\newfontfamily\emoji{Symbola}

\chapter{Those Other Languages}
\label{ch:languages}

\epigraph{New York 1. Act making it a misdemeanor to make a speech or talk in public manner, in any language other than English upon any subject relating to the form of a character of the government or the administration or enforcement of the laws of this state or the United States.}{\itshape Introduced in the Assembly by Mr Hamill, Feb.23, and referred to the Codes Committe (A.878.)}

\parindent=1em



\section{The world's scripts and languages}


On May 23, 1918, Iowa Gov. William Harding banned the use of any foreign language in public: in schools, on the streets, in trains, even over the telephone. Frese  \footfullcite{frese} published a detailed history of this event in American history during the Great War years and its effects, which can even today be seen in Iowa. Such events of course during stressful times in a history of a country are not unique to America and similar politics can be observed throughout all human history. Today most Americans' response to the calling of such a law would probably be the Unicode Character \unicodenumber{U+1F4A9}\footnote{\protect\emoji\protect\char"1F4A9 self describing!}. Getting the character to print as a footnote in a document is another story. As many of the world's languages are facing extinction and the inclusion of a section in the |phd| package to deal with different scripts and appropriate fonts has been done in this spirit.

 
Probably there are more users of \latexe whose mother tongue is not English than those who speak the language. \tex out of the box does not offer facilities for using non-latin based scripts easily; this presents numerous problems. The biggest problem---which has been solved to a large extent---was the entering of text without having to mark all the special
characters such as umlauts (\"o) with commands. The second issue and which has been addressed by packages such as Babel, is redefining the strings such as ``Chapter" to another language. In software this is called internationalization and a governing standard is |i18n|. None of the current packages take such an approach and none of them as yet offer a satisfactory solution for |LuaLaTeX|. 


Another issue with writing systems and scripts is finding and using appropriate fonts. Most writing systems that have ever existed are now extinct. Only minute vestiges of one of the most ancient---Egyptian hieroglyphs---live on, unrecognized, in the Latin alphabet in which English, among hundreds of other languages, is conveyed today. The latin \textit{m}, for example, ultimately derives from the Egyptian's n-sign, depicting waves. There may never be a font that includes all the unicode characters (code2000) came close. Good fonts with well over ten thousand characters, keyed to the Unicode system, are now readily available. 

Bringhurst in the Elements of Typographic Style \footcite{Bringhurst2005} critisized the allotment of only 256 characters in the extended |ASCII| specification and other software and considered this practice by software developers as \enquote{typographically sectarian and culturally stunted}. 


Bringhurst comments were unfair to programmers as he was probably unaware of the difficulties. Many  scripts are widely different to the Latin script. Hanunó'o is written vertically from bottom to top, whereas tibetan and sometimes Chinese from top to bottom.  Middle Eastern scripts such as Hebrew and Arabic are written from right to left. Some of the scripts have other peculiarities as they take different forms when they are at the middle of a word or at the end. Ancient scripts such as hieroglyphics could be written from top to botton or from right to left or left to right or boustrostrephon. The glyphs of the latter could also face either left or right and the writing direction can be determined based on the direction the figures are \enquote{seeing}.\index{boustrostrephon}

Note that  we will be using the word ``script" instead of a ``writing system". Many people associate the word ``script" with a small program which is normally used on the command line. Here ``script" means a collection of letters and other characters, meant for writing human languages in a systematic way.  We say that languages such as English, Dutch and Icelandic and Vietnam use the Latin \emph{script}, although they have different repertoires of characters. 


\section{TeX's support for different languages}

\tex's support for languages centered around hyphenation patterns.
Primitives such as \docAuxCommand{language}=\meta{number} can be used to store hyphenation patterns and exceptions for up to 256 different languages. 
This primitive is then used by \tex to apply an appropriate set of hyphenation rules for each paragraph or part of a paragraph in a document\footnote{\url{http://www.tug.org/utilities/plain/cseq.html language-rp}}. 

When \tex begins a ne paragraph it sets the \emph{current language} to \cs{language}. Just before it adds each new character to the paragraph in unrestricted horizontal mode, it compares the current language to \cmd{\language}. If they are different, TeX : 

\begin{enumerate}{}

\item changes the current language to \cmd{\language}; 

\item inserts a whatsit\index{whatsit>language} containing the new language and the values of |\lefthyphenmin| and |\righthyphenmin|; 

\item inserts the character. The |\setlanguage| command should be used to change languages in restricted horizontal mode (i.e., inside an |\hbox|). 
\end{enumerate}
If \meta{number} is less than 0 or greater than 255, 0 is used [455].
  Plain TeX has a \docAuxCommand{newlanguage} command which may be used to allocate numbers for languages [347]. Changes made to \refCom{language} are local to the group containing the change 
  
If you enter, for example, |\newlanguage\Catalan|, then to switch to the hyphenation patterns of the Catalan language, you need to write |\language = \Catalan|. Writing |\Catalan| by itsef is not sufficient. 
More about \tex's support for languages can be found in the \nameref{ch:hypenation}

\section{LaTeX language management}

\latexe follows the same route as \tex and Plain TeX and its only language support is for hyphenation.
In the source2e the File |lthyphen.dtx| describes the approach to loading the default file |hyphen.ltx| . If a file hyphen.cfg is found \latexe will load the appropriate hyphenation patterns. 

Traditionally language management was achieved using Johan 
Braams package \pkg{Babel} which we describe in the next section. Numerous packages to assist in using different languages with \latex can be found at \url{http://www.ctan.org/tex-archive/language/}. 

\section{The Babel Package} 

The package \pkg{Babel} developed by \footfullcite{babel} was the first package to systematically offer foreign language
support for \latex. It has been updated for use with \XeTeX\ and \LuaTeX\ and provides an environment
in which documents can be typeset in a language
other than US English, or in more than one language.
However, no attempt has been done to
take full advantage of the features provided by the
latter, which would require a completely new core
(as for example polyglossia or as part of a future \latex3).

\subsection{Language files}
The package has a number of predefined language files with the extension |ldf|. Each \emph{language definition file} contains commands appropriate for setting strings and hyphenation patterns in the particular language, as well as
many ancillary macros to typeset dates and numbers in the typographical convention of the language. 


\begin{docCommand} {selectlanguage} {\marg{language}} {default none, initial US English}
When a user wants to switch from one language to another he can
do so using the macro |\selectlanguage|. This macro takes the
language, defined previously by a language definition file, as
its argument. It calls several macros that should be defined in
the language definition files to activate the special definitions
for the language chosen. For ``historical reasons'', a macro name is
converted to a language name without the leading |\|; in other words,
the two following declarations are equivalent:
\end{docCommand}
\begin{phdverbatim}
\selectlanguage{german}
\selectlanguage{\german}
\end{phdverbatim}

\begin{docCommand}{foreignlanguage}{\marg{language}\marg{text}}
The command |\foreignlanguage| takes two arguments; the second
argument is a phrase to be typeset according to the rules of the
language named in its first argument. This command (1) only
switches the extra definitions and the hyphenation rules for the
language, \emph{not} the names and dates, (2) does not send
information about the language to auxiliary files (i.e., the
surrounding language is still in force), and (3) it works even if
the language has not been set as package option (but in such a
case it only sets the hyphenation patterns and a warning is shown).
\end{docCommand}

\begin{docCommand}{otherlanguage*} { \marg{language}{otherlanguage*}}
Same as |\foreignlanguage| but as environment. Spaces after the
environment are \textit{not} ignored.
\end{docCommand}


\section{The Polyglossia package}

The \pkg{polyglossia} package has a lot of potential and has solved many issues
but its integration with large parts of the traditional |pdfLaTeX| world
is still under development and will probably take a while before one could
declare it easy to use and bug free \footfullcite{polyglossia}. For example anything with the |bidi| package has issues with loading orders for a number of packages and least of which is with
the Ams packages. So if you are going to mix a number of languages in a \XeTeX\ document
you need to take extra care.

 Polyglossia is a package for facilitating multilingual typesetting with
 \XeLaTeX\ and (at an early stage) \LuaLaTeX.  Basically, it
 can be used as a replacement of \pkg{babel} for performing the following
 tasks automatically:
 
 \begin{enumerate}
 \item Loading the appropriate hyphenation patterns.
 \item Setting the script and language tags of the current font (if possible and
       available), via the package \pkg{fontspec}.
 \item Switching to a font assigned by the user to a particular script or language.
 \item Adjusting some typographical conventions according to the current language
       (such as afterindent, frenchindent, spaces before or after punctuation marks,
       etc.).
 \item Redefining all document strings (like chapter, ``figure'', ``bibliography'').
 \item Adapting the formatting of dates (for non-Gregorian calendars via external
       packages bundled with polyglossia: currently the Hebrew, Islamic and Farsi
       calendars are supported).
 \item For languages that have their own numbering system, modifying the formatting
       of numbers appropriately (this also includes redefining the alphabetic sequence
       for non-Latin alphabets).\footnote{ %
         For the Arabic script this is now done by the bundled package \pkg{arabicnumbers}.}
 \item Ensuring proper directionality if the document contains languages
       that are written from right to left (via the package \pkg{bidi},
       available separately).
 \end{enumerate}
 
 Several features of \pkg{babel} that do not make sense in the \XeTeX\/\luatex world (like font
 encodings, shorthands, etc.) are not supported supported by the package.
 
 Generally speaking, \pkg{polyglossia} aims to remain as compatible as possible
 with the fundamental features of \pkg{babel} while being cleaner, light-weight,
 and modern. The package \pkg{antomega} has been very beneficial in our attempt to
 reach this objective.


\section{Loading language definition files}

The recommended way of \pkg{polyglossia} to load language definition files
is given in the manual as:
 
\begin{docCmd}{setdefaultlanguage}{\oarg{options}\marg{lang}}
 (or equivalently \cmd\setmainlanguage).
\end{docCmd}
 
 Secondary languages can be loaded with

\begin{docCmd} {setotherlanguage}{\oarg{options}\marg{lang}}
\end{docCmd}
 These commands have the advantage of being explicit and of allowing you to set
 language-specific options.\footnote{ %
 More on language-specific options below.}
 It is also possible to load a series of secondary languages at once using

\begin{docCmd}{setotherlanguages} { \marg{lang1,lang2,lang3,\ldots}}
\end{docCmd}

 Language-specific options can be set or changed at any time by means of
\begin{docCmd}{setkeys} { \marg{lang}\marg{opt1=value1,opt2=value2,\ldots}}
\end{docCmd}

\subsection{Bidirectional languages}





\begin{comment}
\begin{Arabic}
ّ هو إذ الغاية؛ شريف الفوائد، جم المذهب، عزيز فنّ التاريخ فنّ أنّ اعلم
والملوك سيرهم، في والأنبياء أخلاقهم، في الأمم من الماضين أحوال على يوقفنا
ّ أحوال في يرومه لمن ذلك في الإقتداء فائدة تتم حتّى وسياستهم؛ دولهم في
والدنيا. الدين
\end{Arabic}
\end{comment}

The Greek language is represented both in modern Greek as well as its ancient variants.

\begin{teX}
\begin{greek}
\textbf{Η ελληνική γλώσσα} είναι μία από τις ινδοευρωπαϊκές γλώσσες, για την
οποία έχουμε γραπτά κείμενα από τον 15ο αιώνα π.Χ. μέχρι σήμερα. Αποτελεί το
μοναδικό μέλος ενός κλάδου της ινδοευρωπαϊκής οικογένειας γλωσσών. Ανήκει
επίσης στον βαλκανικό γλωσσικό δεσμό.\\	
\end{greek}
\end{teX}

\topline

\textbf{Η ελληνική γλώσσα} είναι μία από τις ινδοευρωπαϊκές γλώσσες, για την
οποία έχουμε γραπτά κείμενα από τον 15ο αιώνα π.Χ. μέχρι σήμερα. Αποτελεί το
μοναδικό μέλος ενός κλάδου της ινδοευρωπαϊκής οικογένειας γλωσσών. Ανήκει
επίσης στον βαλκανικό γλωσσικό δεσμό.\\	

\bottomline

\begin{verbatim}
\begin{russian}
\textbf{Русский язык} — один из восточнославянских языков, один из 
крупнейших языков мира, в том числе самый распространённый из славянских
языков и самый распространённый язык Европы, как географически, так и по
числу носителей языка как родного (хотя значительная, и географически бо́
льшая, часть русского языкового ареала находится в Азии).	\\

\end{russian}
\end{verbatim}



\textbf{Русский язык} — один из восточнославянских языков, один из крупнейших языков мира, в том числе самый распространённый из славянских языков и самый распространённый язык Европы, как географически, так и по числу носителей языка как родного (хотя значительная, и географически бо́льшая, часть русского языкового ареала находится в Азии).	\\


\section{The Translator package}

The \pkg{translator} package was developed by Till Tantau \cite{translator}. It provides a flexible
mechanism for translating individual words into different languages.
For example, it can be used to translate a word like \enquote{figure} into,
say, the German word \enquote{Abbildung}. Such a translation mechanism is
useful when the author of some package would like to localize the
package such that texts are correctly translated into the language
preferred by the user. The translator package is \emph{not} intended
to be used to automatically translate more than a few words. 

You may wonder whether the translator package is really necessary
since there is the (very nice) |babel| package available for
\LaTeX. This package already provides translations for words like
``figure''. Unfortunately, the architecture of the babel package was
designed in such a way that there is no way of adding translations of
new words to the (very short) list of translations directly build into
babel.

The translator package was specifically designed to allow an easy
extension of the vocabulary. It is both possible to add new words that
should be translated and translations of these words.

\subsection{Using the Translator Package}

  The \pkg{Translator}\footcite{translator} needs to be used with \pkg{Babel} and I am not too sure yet 
  if it is ready  to be used with Polyglossia.

Once the package has loaded a language or a set of languages the optional argument to the
\cmd{\translate} can be used to translate a string. 

\begin{texexample}{Translating strings}{ex:translator}
  \translate[to=german]{author}
  \translate[to=dutch]{author}
\end{texexample}

Before you can provide the translations you need to provide your own dictionaries, where you require them. These need to be installed at a place where \tex can find them.

\begin{docCmd} {ProvidesDictionary} { \marg{dictionary file name} \marg{language} }
\end{docCmd}

The dictionary has to be saved in a specific format that relates to the \refCmd{ProvidesDictionary} command. The second argument of the command must be appended to the file name; for the example the file is saved as\footnote{This  example is from the translator package bundle and is under the folder \texttt{base}}:

|translator-basic-dictionary-German|

The concepts take a bit of time to sink in, but once you have everything set up, it is quite easy and straight forward to incorporate it, into your package. 

\begin{teXXX}
\ProvidesDictionary{translator-basic-dictionary}{German}

\providetranslation{Abstract}{Zusammenfassung}
\providetranslation{Addresses}{Adressen}
\providetranslation{addresses}{Adressen}
\providetranslation{Address}{Adresse}
\providetranslation{address}{Adresse}
\providetranslation{and}{und}
\providetranslation{Appendix}{Anhang}
\providetranslation{Authors}{Autoren}
\providetranslation{authors}{Autoren}
\providetranslation{Author}{Autor}
\providetranslation{author}{Autor}
\end{teXXX} 

This is in contrast to Babel and Polyglossia that define
commands for each string to be translated such as,

\begin{phdverbatim}
\def\captionsdutch{%
    \def\prefacename{Voorwoord}%
    \def\refname{Referenties}%
    \def\abstractname{Samenvatting}%
    \def\bibname{Bibliografie}%
    \def\chaptername{Hoofdstuk}%
    \def\appendixname{Bijlage}%
    ...
    \def\proofname{Bewijs}%
    \def\glossaryname{Verklarende woordenlijst}%
    \def\today{\number\day~\ifcase\month%
      \or januari\or februari\or maart\or april\or mei\or juni\or
      juli\or augustus\or september\or oktober\or november\or
      december\fi
      \space \number\year}}
\end{phdverbatim}

\begin{docCommand}{usedictionary}{\marg{kind}}
  This command tells the |translator| package, that at the beginning of
  the document it should load \textit{all} dictionaries of kind \meta{kind} for
  the languages used in the document. Note that the dictionaries are
  not loaded immediately, but only at the beginning of the document.

  If no dictionary of the given \emph{kind} exists for one of the
  language, nothing bad happens.

  Invocations of this command accumulate, that is, you can call it
  multiple times for different dictionaries.
\end{docCommand}

\begin{docCommand}{uselanguage}{\marg{list of languages}}
  This command tells the |translator| package that it should load the
  dictionaries for all languages in the \meta{list of languages}. The
  dictionaries are loaded at the beginning of the document.
\end{docCommand}



\chapter{CLDR}

The \pkg{phd} package provides facilities for language handling, but albeitly still at an experimental stage. Sectioning command strings can easily be set in one's language by just typing the key in the appropriate language.

\begin{texexample}{Example of changing language in headings}{ex:lheadings}
\bgroup
\cxset{locale turkish,
       chapter format=block,
      chapter opening=anywhere,
       chapter number color = black,
      }
\chapter{Testing}
        
\egroup
\end{texexample}


The language message text are actually variables (pretty much similar to the message modules of the l3error package. It follows
patterns for defining such messages in other languages and in the Linux kernel. (|get_text|). Actually your error messages
in packages belong here. 

These resources should preferably be put into files that wil be loaded by a library that uses a combination of language and country (also known as the \enquote{locale}) to identfy the right string. Once we have placed these files we can send them to the translation vendor and get back translated files for each locale that your application is going to support.

There are various file formats that make suitable resource files. Popular choices are JSON, XML, gettext or YAML. The translator files that we discussed earlier are very similar in context. 

\begin{phdverbatim}     
     locale/en/names/part~name/.store      = part_name_tl,
     locale/en/names/chapter~name/.store   = chapter_name_tl,
     get_text{en}{chapter_name_tl} -> Chapter
\end{phdverbatim} 

A similar storing technique can be employed for other sections of the CLDR specifications such as delimters:

\begin{phdverbatim}
    locale/en/delimiters/quotation start = “,
    locale/en/delimiters/quotation end =  ”,
    locale/en/delimiters/alternate quotation start = ‘,
    locale/en/delimiters/alternate quotation end = ’,
\end{phdverbatim}

The i18n CLDR discussed in more detailed in the next chapter provide ready made internationally agreed json or xml files
detailing the most common internationalization tasks, such as strings for dates, months, calendars, quotes, common units and
sorting the latter is very important for many tasks, such as bibliographies. Once we have the structure defined
a small Go utility can download all the files and translate them into our resource files. These will be missing the
strings for sectioning, typographical conventions, shorthands and other conventions normally handled by Babel. 

Babel and polyglossia modify the basic LaTeX environments or macros to achieve this. In my opinion it should be the other way out, they should only provide a value to be used by these commands rather than the commands be cloberred at this level.

\section{Numbers}

The formatting of numbers for a locale is specified by the CLDR in files containing the file |numbers|. These can be downloaded as |xml| files or |json| files.
Currently most users of \latexe requiring to format numerals they will use either |numprint| or |SIUnitx|. The latter has many settings as it handles scientific units. For formatting numbers in \latexe the |cldr| specifications and data are somewhat limited. Package options and commands
would normally handle rounding, leading zeroes for decimals, plums minus signs for the combination (+-) and other similar requirements.



\begin{texexample}{Using numprint}{ex:numprint}
% basic command
\numprint{12500678.912345}

% shorter version
\np{12500678.912345}
\end{texexample}



\paragraph{Numbering systems}

Numbering systems are used to show different representations of numeric values. Each numbering system consists of characters that represent numeric digits. In addition, there are also number symbols used with each numbering system that may differ when the numbering system is used in different locales.

The default numbering system for a locale is the numbering system that is normally used to represent numbers in that locale.

\begin{verbatim}
"numbers": {
        "defaultNumberingSystem": "latn",
        "otherNumberingSystems": {
          "native": "latn"
        },
        "minimumGroupingDigits": "1",
        "symbols-numberSystem-latn": {
          "decimal": ".",
          "group": ",",
          "list": ";",
          "percentSign": "%",
          "plusSign": "+",
          "minusSign": "-",
          "exponential": "E",
          "superscriptingExponent": "×",
          "perMille": "‰",
          "infinity": "∞",
          "nan": "NaN",
          "timeSeparator": ":"
        },
\end{verbatim}

The native numbering system for a locale is the numbering system used for native digits, and is normally in the script for the locale's language. Native numbering systems can only use numeric positional decimal digits, like for Latin numbers (0123456789). If the numbering system in your language uses an algorithm to spell out numbers in the language's script, label it as a traditional numbering system instead. The traditional numbering system does not need to be specified if it is the same as the native numbering system.

The default, native and traditional numbering systems for a locale may be different. For example, in Tamil the default numbering system is |latn|, the native numbering system is |tamldec| and the traditional numbering system is |taml|.

\begin{trivlist}\item[]
\begin{tabular}{lll}
\toprule
Code	 & Description	 & Digits\\
\midrule
arab	 & Arabic-Indic digits	&\panunicode ٠١٢٣٤٥٦٧٨٩\\
fullwide &   	Full width digits &\panunicode 	０１２３４５６７８９\\
hant	   & Traditional Chinese numerals — non-decimal	& algorithmic\\
latn	   &Latin digits	 &0123456789\\
\bottomrule
\end{tabular}
\end{trivlist}

\paragraph{Minimum digits for grouping}

In some languages, the grouping separator is suppressed in certain cases. For example, see china-auf-wachstumskurs.gif, where there is a grouping separator in \enquote{12 080} but not in \enquote{4720}. The |minimumGroupingDigits| determines what the default for a locale is. In this case the value should be \enquote{2} to illustrate that the separator only appears once the number of thousands goes into the double-digits (i.e. 10 thousand or above) and not for single-digit thousands (i.e. anything below 10 thousand).


Note that this is just the default, and the grouping separator may be retained in lists, or removed in other circumstances. For example, in English the \enquote{,} is used by default, but not in addresses (\enquote{12345 Baker Street}), in 4-digit years (2014, but 12,000 BC), and certain other cases.

\begin{texexample}{Numprint minimum grouping}{ex:numprint2}
\begingroup
\npfourdigitnosep$\numprint{1234.1234}$, $\numprint{12345.12345}$ 

\npfourdigitsep$\numprint{1234.1234}$, $\numprint{12345.12345}$
\endgroup
\end{texexample}

The much larger package \pkg{siunitx} can also be used to parse and typeset numbers in different formats, using the command \docAuxCommand{num}.


\begin{texexample}{siunitx}{ex:siunitx}
\num{123}\\
\num{1234}\\
\num{12 345}\\
\num{0.123} \\
\num{0,1234}\\
\num{.12345}\\
\num{3.45d-4}\\
\num{-e10}
\end{texexample}

The package also provides commands for formatting angles, ranges and similar. For the latter it also provides a limited set of localization commands by using the \pkg{translator} from the \pkg{Beamer} bundle.

The package defines numerous keys that can be used either at package level or as options to the command num to format and print the numbers. In the next example the group separator is set uing the key |group-separator|. 

\begin{texexample}{siunitx group separator}{ex:siunitx-01}
\num{12345} \\
\num[group-separator = {,}]{12345} \\
\num[group-separator = \text{~}]{12345}
\end{texexample}


\paragraph{Number Symbols} The following symbols are used in formatting numbers. They will be substituted for the placeholders in Number Patterns. 

\begin{longtable}{llp{5cm}}
\toprule
Name	&English Example	&Meaning\\
\midrule
|decimal|	  &2,345.67	 &decimal separator\\
|group|	     &2,345.67	 &grouping separator, typically for thousands\\
|minusSign|  &	+23	*	 &the plus sign used with numbers\\
|plusSign|	  &-23	*	    &the minus sign used with numbers\\ 
|perMille|	  &234‰	*	&the permille sign (out of 1000)\\
|exponential|	      &1.2E3	*	&used in computers for 1.2×10³.\\
|superscriptingExponent|	&1.2×103	* &human-readable format of exponential \\
|infinity|	  &∞	*	&used in +∞ and -∞.\\ 
|nan|	     &NaN	*	&\enquote{not a number}. \\
\bottomrule
\end{longtable}


%The + and - symbols are intended for unary usage, and not for binary usage; thus represents either the positive number or a negative number. For example, in an operation 3 -(-2), the defined symbol would be used for the second minus sign, but not for the subtraction operator. Any directionality markers needed (e.g. <LRM>) to keep with the number should be included.
%percentSign	23.4%%	*	the percent sign (out of 100)

\paragraph{Number Patterns}

Numbers are formatted using patterns, like |#,###.00|. For example, the pattern |#,###.00| when used to format the number 12345.678 could result in "12'345,67". That would happen if the grouping separator for your language is an apostrophe ('), and the decimal separator is a comma (,).  Also see Number Symbols.

Important: The characters . , 0 \# (and others below) are special placeholders; they stand for the decimal separator, and so on, and are NOT real characters. You must NOT "translate" the placeholders; for example, don't change '.' to ',' even though in your language the decimal point is written with a comma.

Here are the special characters used in number patterns.

Whenever any of these symbols are in the English pattern, they must be retained in the pattern for your language. The positions of some of them (\%, ¤) may be changed, or spaces added or removed. The symbols will be replaced by the local equivalents, using the Number Symbols for your language. Verify results by reviewing the dynamic examples in the right-hand pane.


The cldr locale files, provide these patterns. They can then be used to format general purpose numbers, which fall into
five categories.

\begin{longtable}{p{2.5cm}lp{6.5cm}}
\toprule
Type	&English Example	& Meaning\\
\midrule
currency	&¤|#,##0.00|  &Used for currency values. A currency symbol (¤) is will be replaced by the appropriate currency symbol for whatever currency is being formatted. The choice of whether to use the international currency symbols (USD, EUR, JAY, RUB,…) or localized symbols (\$, €, ¥, руб.,…) is up to the application program that uses CLDR. Note: the number of decimals will be set by programs that use CLDR to whatever is appropriate for the currency, so do not change them; keep exactly 2 decimals.\\

currency-accounting	 &¤|#,##0.00|;(¤|#,##0.00|)	&Used for currency formats in accounting contexts.\\
\bottomrule
\end{longtable}

Pattern Characters are shown below.

\begin{longtable}{lp{11cm}}
\caption{Pattern Characters}\\
\toprule
Symbol & Meaning\\
\midrule
.	&Replaced automatically by the character used for the decimal point in your language. Not a real period; must be retained!\\
,	&Replaced by the "grouping" (thousands) separator in your language. Not a real comma; must be retained!\\
0	&Replaced by a digit (or zero if there aren't enough digits).\\
\#	&Replaced by a digit (or nothing if there aren't enough). Often used to show the position of the ",".\\
¤	&This will be replaced by a currency symbol, such as \$ or USD. Note: by default a space is automatically added between letters in a currency symbol and adjacent numbers. So you don't need to add a space between them if your language writes \enquote{\$12} but \enquote{USD 12}.\\
\%	&This marks a percent format. The \% symbol may change position, but must be retained.\\
E	&This marks a scientific format. The E symbol may change position, but must be retained.\\
'	&If any of the above characters are used as literal characters, they must be quoted with ASCII single quotes. For example, in the Short Numbers if a period needs to be used to mark an abbreviation, it would appear as:
0.0 tis'.'
not
0.0 tis.\\
\ldots;\ldots	&If your language uses different formats for negative numbers than just adding "-" at the front, you can put in two patterns, separated by a semicolon. The first will be used for zero and positive values, while the second will be used for negative values.
For example: |#,##|0.00¤;(|#,##|0.00¤) is used to make negative currencies appear like \enquote{(1'234,56£)} instead of \enquote{-1'234,56£}. That is used for formatting currency amounts in English, but not for general-purpose decimal numbers.\\
\bottomrule
\end{longtable}

\section{Characters}
The |<characters>| element provides optional information about characters that are in common use in the locale, and information that can be helpful in picking resources or data appropriate for the locale, such as when choosing among character encodings that are typically used to transmit data in the language of the locale. It typically only occurs in a language locale, not in a language/territory locale.

\begin{quote}
|<exemplarCharacters>[a-zåæø]</exemplarCharacters>|
\end{quote}

The exemplar character set contains the commonly used letters for a given modern form of a language, which can be for testing and for determining the appropriate repertoire of letters for charset conversion or collation. ("Letter" is interpreted broadly, as anything having the property Alphabetic in the [UCD], which also includes syllabaries and ideographs.) It is not a complete set of letters used for a language, nor should it be considered to apply to multiple languages in a particular country. Punctuation and other symbols should not be included.

There are two sets: the main set should contain the minimal set required for users of the language, while the auxiliary exemplar set is designed to encompass additional characters: those non-native or historical characters that would customarily occur in common publications, dictionaries, and so on. So, for example, if Irish newspapers and magazines would commonly have Danish names using å, for example, then it would be appropriate to include å in the auxiliary exemplar characters; just not in the main exemplar set. Major style guidelines are good references for the auxiliary set. Thus for English we have [a-z] in the main set, and [á à ă â å ä ā æ ç é è ĕ ê ë ē í ì ĭ î ï ī ñ ó ò ŏ ô ö ø ō œ ß ú ù ŭ û ü ū ÿ] in the auxiliary set.

In general, the test to see whether or not a letter belongs in the main set is based on whether it is acceptable in that language to always use spellings that avoid that character. For example, the exemplar character set for en (English) is the set [a-z]. This set does not contain the accented letters that are sometimes seen in words like "résumé" or "naïve", because it is acceptable in common practice to spell those words without the accents. The exemplar character set for fr (French), on the other hand, must contain those characters: [a-z é è ù ç à â ê î ô û æ œ ë ï ÿ]. The main set typically includes those letters commonly taught in schools as the "alphabet".

The list of characters is in the Unicode Set format, which allows boolean combinations of sets of letters, including those specified by Unicode properties.

Sequences of characters that act like a single letter in the language — especially in collation — are included within braces, such as [a-z á é í ó ú ö ü ő ű \{cs\} \{dz\} \{dzs\} \{gy\} \ldots]. The characters should be in normalized form (NFC). Where combining marks are used generatively, and apply to a large number of base characters (such as in Indic scripts), the individual combining marks should be included. Where they are used with only a few base characters, the specific combinations should be included. Wherever there is not a precomposed character (e.g. single codepoint) for a given combination, that must be included within braces. For example, to include sequences from the Where is my Character? page on the Unicode site, one would write: [\{ch\} \{tʰ\} \{x̣\} \{ƛ̓\} {ą́} {i̇́} {ト゚}], but for French one would just write [a-z é è ù ...]. When in doubt use braces, since it does no harm to included them around single code points: e.g. [a-z \{é\} \{è\} \{ù\} ...].

If the letter 'z' were only ever used in the combination 'tz', then we might have [a-y {tz}] in the main set. (The language would probably have plain 'z' in the auxiliary set, for use in foreign words.) If combining characters can be used productively in combination with a large number of others (such as say Indic matras), then they are not listed in all the possible combinations, but separately, such as:

{\panunicode [‌ ‍ ॐ ०-९ ऄ-ऋ ॠ ऌ ॡ ऍ-क क़ ख ख़ ग ग़ घ-ज ज़ झ-ड ड़ ढ ढ़ ण-फ फ़ ब-य य़ र-ह ़ ँ-ः ॑-॔ ऽ ् ॽ ा-ॄ ॢ ॣ ॅ-ौ] }

The exemplar character set for Han characters is composed somewhat differently. It is even harder to draw a clear line for Han characters, since usage is more like a frequency curve that slowly trails off to the right in terms of decreasing frequency. So for this case, the exemplar characters simply contain a set of reasonably frequent characters for the language.

The ordering of the characters in the set is irrelevant, but for readability in the XML file the characters should be in sorted order according to the locale's conventions. The set should only contain lower case characters (except for the special case of Turkish and similar languages, where the dotted capital I should be included); the uppercase letters are to be mechanically added when the set is used. For more information, see [Data Formats] and the discussion of Special Casing in the Unicode Character Database.

For example for the locale |se| for Northern Sami, we have:


\begin{longtable}{l p{8cm}}
\toprule
Attribute             & Value \\
\midrule
exemplar characters   & a á b c č d đ e f g h i j k l m n ŋ o p r s š t ŧ u v z ž\\
exemplar characters auxiliary  & à ç é è í ń ñ ó ò q ú w x y ü ø æ å ä ã ö\\
exemplar characters index  &A Á B C Č D Đ E É F G H I J K L M N Ŋ O P Q R S Š T Ŧ U V W X Y Z Ž Ø Æ Å Ä Ö\\
exemplar characters numbers &  , \% ‰ + − 0 1 2 3 4 5 6 7 8 9\\
\bottomrule
\end{longtable}


\begin{longtable}{l p{8cm}}
\toprule
Attribute             & Value \\
\midrule
 exemplar characters &a b c ç d e f g ğ h ı i İ j k l m n o ö p r s ş t u ü v y z\\
exemplar characters  auxiliary & á à ă â å ä ã ā æ é è ĕ ê ë ē í ì ĭ î ï ī ñ ó ò ŏ ô ø ō œ q ß ú ù ŭ û ū w x ÿ\\
 exemplar characters index & A B C Ç D E F G H I İ J K L M N O Ö P Q R S Ş T U Ü V W X Y Z\\
exemplar character numbers & \- , . \% ‰ + 0 1 2 3 4 5 6 7 8 9\\
exemplar characters punctuation &  - ‐ – — , ; : ! ? . … ' ‘ ’ " “ ” ( ) [ ] § @ * / \& \# † ‡ ′ ″\\
\bottomrule
\end{longtable}


\paragraph{ellipsis}The ellipsis element provides patterns for use when truncating strings. There are three versions: initial for removing an initial part of the string (leaving final characters); medial for removing from the center of the string (leaving initial and final characters), and final for removing a final part of the string (leaving initial characters). For example, the following uses the ellipsis character in all three cases (although some languages may have different characters for different positions).

\begin{longtable}{ll}
Ellipsis final & \{0\}… \\           
Ellipsis initial & …\{0\} \\         
Ellipsis medial  & \{0\}…\{1\} \\       
Ellipsis word-final & \{0\} … \\     
Ellipsis word-initial & … \{0\} \\   
Ellipsis word-medial & \{0\} … \{1\}\\
\end{longtable} 

\paragraph{List patterns} List patterns can be used to format variable-length lists of things in a locale-sensitive manner, such as \enquote{Monday, Tuesday, Friday, and Saturday} (in English) versus \enquote{lundi, mardi, vendredi et samedi} (in French). For example, consider the following example:

\begin{phdverbatim}
  <listPatterns>
    <listPattern>
	   <listPatternPart type="start">{0}, {1}</listPatternPart>
		<listPatternPart type="middle">{0}, {1}</listPatternPart>
		<listPatternPart type="end">{0}, and {1}</listPatternPart>
		<listPatternPart type="2">{0} and {1}</listPatternPart>
    </listPattern>
		<listPattern type="or">
			<listPatternPart type="start">{0}, {1}</listPatternPart>
			<listPatternPart type="middle">{0}, {1}</listPatternPart>
			<listPatternPart type="end">{0}, or {1}</listPatternPart>
			<listPatternPart type="2">{0} or {1}</listPatternPart>
	</listPattern>
	<listPattern type="unit">
			<listPatternPart type="start">{0}, {1}</listPatternPart>
			<listPatternPart type="middle">{0}, {1}</listPatternPart>
			<listPatternPart type="end">{0}, {1}</listPatternPart>
			<listPatternPart type="2">{0}, {1}</listPatternPart>
	</listPattern>
	<listPattern type="unit-narrow">
			<listPatternPart type="start">{0} {1}</listPatternPart>
			<listPatternPart type="middle">{0} {1}</listPatternPart>
			<listPatternPart type="end">{0} {1}</listPatternPart>
			<listPatternPart type="2">{0} {1}</listPatternPart>
	</listPattern>
	<listPattern type="unit-short">
			<listPatternPart type="start">{0}, {1}</listPatternPart>
			<listPatternPart type="middle">{0}, {1}</listPatternPart>
			<listPatternPart type="end">{0}, {1}</listPatternPart>
			<listPatternPart type="2">{0}, {1}</listPatternPart>
	</listPattern>
  </listPatterns>
\end{phdverbatim}	

These are not very useful for \tex as most of this type of work can be simply be achieved by just typing the values. the
\pkg{siunitx} offers similar facilities for lists, through the \pkg{translator} and Babel.

\begin{texexample}{clist}{ex:clistuse}
\ExplSyntaxOn
\group_begin:
\def\firsttwowords{~and~}
\def\lasttwowords{ ~and~ }
\def\betweenmorethantwo{ ,~ }
\clist_set:Nn \l_tmpa_clist { a , b , , c , {de} , f }
\clist_use:Nnnn \l_tmpa_clist { \firsttwowords } { ,~ } { ,\lasttwowords }
\group_end:
\ExplSyntaxOff
\end{texexample}


\paragraph{Typographical considerations and Convenience Commands} Some commands provided by babel-french are intended to make typesetting according to French typographical conventions easier. Some twenty three conditionals, which more or less affect typographical rules or conventions are mentioned in Babel (see p.43, frencgb.pdf).

    \begin{enumerate}
       \item Hyphenation parameters such as lefthyphenmin and righthyphenmin are defined for many of the languages.
             
             \begin{tabular}{lll}
             \toprule
               Language        & \cs{lefthyphenmin} & \cs{righthyphenmin}\\
             \midrule  
               Finnish         &    2               & 2                   \\
               French          &2                   & 2                   \\
             \bottomrule  
             \end{tabular}
       \item Delimiters (Quotation marks): Delimiters according to CLDR terminology are the characters used for quoting texr. For example in UK English they are the \enquote{curly} right and left forms as in this \enquote{this phrase}. the alternate forms are for embedded quotations such as \enquote{He yelled \enquote{Stop!}, and turned around.} Babel for many of the languages provides macros to enclose text in quotes|\og| and |\fg|.
       \item Typesetting of superscripts such as nth etc. In the French section of Babel this is defined as \docAuxCommand{up}, used
             as M|\up|me \foreignlanguage{french}{M\up{me}}.
       \item French spacing. 
       \item Spacing before punctuation. With Babel and LuaLaTeX a lua script is loaded, that uses callbacks to intercept
             the punctuation and add the appropriate node attributes. The callbacks are fairly comprehensive and cater for
             some edge cases such as 1sp columns etc.
       \item For the other engines it falls back to active characters or to XeTeX character classes. 
       \item Caption separators. In French, captions in figures and tables should never be printed as \enquote{Figure 1:} which is the default in standard \latexe classes; the \enquote{:} is made active too late, no space is added befre it. With \lualatex and \xelatex, this glitch does not occur if you use Babel, you should get \enquote{Figure 1\thinspace:} which is correct in French. 
       \item \textit{Ellipsis Patterns}.  Ellipsis patterns are used in a display when the text is too long to be shown. It will be used in environments where there is very little space. \tex traditionally provided \docAuxCommand{ldots}. With unicode it should be just one character; and where that really can't work, the CLDR specification mentions that it should be as short as possible. 

There are three different possible patterns that need to be translated. Typically the same character is used in all three, but three choices are provided just in case different characters would be appropriate in different contexts, for some languages.

       Babel provides for French macros and switches to allow for the extra spacing required in French typography.

       \item The \pkg{bigfoot} package deeply changes the way footnotes are handled, including providing its own output routine. When |bigfoot| is loaded babel-french drops the customization of footnotes. The layout of footnotes does not depend on the language, as babel's documentation state, it will look wrong if if two footnotes on the same page are looking different because one was called in a French part, the other one in English.  The rest of the code deals in detail as to how to handle the various
       packages and footnotemark.  
     \end{enumerate}
   
   
\paragraph{Babel shorthands}      
My biggest concern with Babel is the ordering of packages due to all the redefinitions and in having to execute most of the code at the |AtBeginDocument| hook.      
    
The way the |PHD| package works is that the user will be provided with a style file, providing all the settings. These can be named. For example |thesis|. Such a style file can be easily be changed to |thesis french|, where for example the field \docAuxKey[phd]{caption separator}{} is set to |caption separator=french colon|.     

\section{Fonts for all the world's scripts and languages}

\epigraph{If you steal from one author it's plagiarrism, if you steal from many, it's research.}{
---Wilson Mizner}

Besides the issues with different languages, hyphenation and caption names, there is also the difficulties with fonts. Unless the current font has the necessary glyphs it will either print junk characters or we get the unicode no glyph symbol.

Many commercial as well as open source fonts exist that can be used to typeset text the world's scripts and languages. The aim of this section of the documentation is to present an overview of the most common scripts represented in the Unicode~7.0 standard. All the examples require the use of the \XeTeX\ or \LUATEX engine. In addition you need to have a copy of the font on your own system. If you do not have them, the font loading mechanism of \XeTeX\ or \LUATEX will take some time to search all the directories and slows compilation tremendously. 

\subsection{Pan-Unicode Fonts}

Thousands of fonts exist on the market, but fewer than a dozen fonts—sometimes described as ``pan-Unicode" fonts—attempt to support the majority of Unicode's character repertoire. Instead, Unicode-based fonts typically focus on supporting only basic |ASCII| and particular scripts or sets of characters or symbols. Several reasons justify this approach: applications and documents rarely need to render characters from more than one or two writing systems; fonts tend to demand resources in computing environments; and operating systems and applications show increasing intelligence in regard to obtaining glyph information from separate font files as needed, i.e. font substitution. Furthermore, designing a consistent set of rendering instructions for tens of thousands of glyphs constitutes a monumental task; such a venture passes the point of diminishing returns for most typefaces.

The \texttt{NotoSerif} fonts from Google\footnote{\protect\url{http://www.google.com/get/noto/}} have good support for 96 language fonts and the list is growing. Since these are widely available most of the scripts that follow use these fonts. Follow the instructions at the website to install them. It is just a matter of dragging them into the fonts folder for most operating systems.

Another freeware pan-Unicode font is \docFont{Titus}
This is an extended version of this font is TITUS Cyberbit Unicode, includes 36,161 characters in v4.0.

On Windows systems |Arial Unicode MS| contains glyphs for all code points within the Unicode Standard version 2.1.  

The code2000 font provides 63546 glyphs and is the nearest font to a universal font to handle Unicode. Unfortunately development stopped in 2008. As a comparison Linux Libertine O, provides 2674 glyphs. \label{code2000}

CJK fonts naturally will have the most glyphs, \docFont{MingLiU} 34046 glyphs and is a very good font for CJK typesetting. Google in conjunction with Adobe also provides a fee CJK font.

The \href{http://ftp.gnu.org/gnu/freefont/}{FreeFont Project} currently supports most of the useful set of free outline (i.e. OpenType) fonts covering as much as possible of the Unicode character set. The set consists of three typefaces: one monospaced and two proportional (one with uniform and one with modulated stroke). 

The idea of having lots of different writing systems into a single font at all? How good does such a font need to be?
There are two extreme views.  The first one is that glyphs in a font shold comprise a unified design entity. This in practice makes sense only within a single language script. Different script systems, such a Latin, Arabic and Devanagari, have different typesetting traditions and conventions.  A good discussion of the advantages and disadvantages can be found at the gnu website \footnote{\protect\url{https://www.gnu.org/software/freefont/articles/Why_Unicode_fonts.html}}. For TeX it is a better proposition in order to avoid switching of fonts that can distract the writer. At least one requires fonts that are inclusive of one's usage. 

\section{The \texttt{ucharclasses} package}

For multilingual texts font switching can become cumbersome. The use of a pan-Unicode font as the default can help. However, if the languages are distinct enough to use different Unicode blocks, which are not covered by the \pkg{polyglossia} package Mike Kamermans' package \pkg{ucharclasses} can be used. This package only works with \xelatex and does not work with LuaTeX. 

\begin{verbatim}
% and the font switching magic
\usepackage[CJK, Latin, Thai, 
           Sinhala, Malayalam, 
           DominoTiles, 
           MahjongTiles]{ucharclasses}
\usepackage{fontspec}

\ifxetex
% default transition uses the widest coverage font I know of
  \setDefaultTransitions{\fontspec{Code2000.ttf}}{}

% overrides on the default rules for specific informal groups
  \setTransitionsForLatin{\fontspec{Palatino Linotype}}{}
  \setTransitionsForCJK{\fontspec{code2000.ttf}}{}%HAN NOM A
  \setTransitionsForJapanese{\fontspec{code2000.ttf}}{}%Ume Mincho

% overrides on the default rules for specific unicode blocks
  \setTransitionTo{CJKUnifiedIdeographsExtensionB}{\fontspec{SimSun-ExtB}}
  \setTransitionTo{Thai}{\fontspec{IrisUPC}}
  \setTransitionTo{Sinhala}{\fontspec{Iskoola Pota}}
  \setTransitionTo{Malayalam}{\fontspec{Arial Unicode MS}}
\ifxetex
\end{verbatim}

{
\newfontfamily\mahjong{FreeSerif.ttf}
\mahjong
domino tiles, 🁇 🀼 🁐 🁋 🁚 🁝, and mahjong tiles: 🀑 🀑 🀑 🀒 🀒 🀒 🀕 🀕 🀕 🀗 🀗 🀗 🀅 🀅 (using FreeSerif)

}

The interaction between Polyglossia and Fontspec can result in infinite looping and memory leaks. I do not recommend that you use these commands as yet. The use of the charclasses will also slow down compilation possibly by a factor of 10.



\section{PhD Settings}

The \pkg{phd} provides support both for scripts, as well as language settings. A script setting sets the system to use appropriate fonts and if the script is associated with a unique language it will automatically handle language settings. Alternatively for multi-language scripts such as the Latin script, the language key can be used. This will automatically setup the language and an appropriate default font. 

\begin{docKey}[phd]{script} { = \meta{script name}} {default none, initial US English}{}
\end{docKey}

\begin{docKey}{language}{ =\meta{language name}}  {default none, US English}
The key language sets the main language for the document. This language will be used for the sectioning commands and common string translations.

If the language is English Polyglossia or Babel are not loaded automatically. If the language is other than English we load either Babel or Polyglossia depending on the engine used.
\end{docKey}


\begin{docKey}{languages}{ = \meta{language1, language2, language3}}  {}
The key |languages|, determines all the other scripts available for typesetting. For each language default font commands are create automatically. The aim is to be able to run a fully multilingual system with the minimum of upfront settings. These we leave to customize in the style template files.
\end{docKey}

\begin{docKey}{greek font}{ = \meta{options}\meta{font file}}  {}
The package comes with numerous language and appropriate default fonts
for each operating system. 
\end{docKey}

\cxset{chapter opening=any}


\section{IPA Transcriptions}

Language is spoken and writing systems need to cater for the individuality of the sounds for a particular language. Many of the world's languages facing extinction do not have a written representation for their language. The \textit{lingua franca} of linguists is the  International Phonetic Alphabet (IPA). This is an alphabetic system of phonetic notation based primarily on the Latin alphabet. It was devised by the International Phonetic Association in the late 19th century as a standardized representation of the sounds of spoken language. The IPA is used by lexicographers, foreign language students and teachers, linguists, speech-language pathologists, singers, actors, constructed language creators and translators. \footcite{ipa}

In the chapters that follow, I have used it extensively. The IPA Handbook is an essential reference work for all those involved in the analysis of speech. Besides the IPA notation a knowledge of linguistic terms is also necessary. A short guide is provided. In \latex the \pkg{tipa} can be of help, but soon a good keyboard layout will be better.

The IPA Extensions block has been present in Unicode since version 1.0, and was unchanged through the unification with ISO 10646. The block was filled out with extensions for representing disordered speech in version 3.0, and Sinology phonetic symbols in version 4.0.[4]

\bigskip
{\catcode`\"=12
\unicodetable{arial}{"0250,"0260,"0270,"0280,"0290,"02A0}
}
\bigskip

\def\schwa{{\arial \char"0259}}

Besides the symbols, there are numerous diacritics and markers.

With Unicode and the right font, there is no problem  in typesetting IPA phonetic symbols. However the problem is the input.

I recommend that you get familiar with a Unicode IPA keyboard overlay. I have used Keyman. When the keyboard is turned on, certian keys (`,@,=) are activated.

As long as your editor allows Unicode input (most do these days) and you're compiling with XeLaTeX or LuaLaTeX, you can just use the IPA keyboard to type directly into the editor just as you can in most other applications. You can also copy and paste your Unicode text from other applications too. 

For example take the transcription of a Hittite word written as \emph{ši-ú-ni-iš}. Here we can typeset it faster by the Hittite package, and numerous others as |\thittite{si-u-ni-is}|. The software is intelligent enough to add the diacritics. They are also expandable. 

\section{The world's scripts}

Anatolian hieroglyphs were first thought to have been used for the Hittite language, 

Anatolian Hieroglyphs is a Unicode block containing Anatolian hieroglyphs, used to write the extinct Luwian language, because they first appeared on personal seals from Hattusha, the capital of the Hittite Empire. While
Hittites did make use of the characters on seals and on their monumental inscriptions, the characters were
used as text primarily for the related language Luwian; a few glosses in Urartian and some divine names
in Hurrian are known to be written in Anatolian Hieroglyphs. Most of the texts are monumental stone
inscriptions, though some letters and accounting documents have been preserved inscribed on strips of
lead. 

\newfontfamily\anatolian{Anatolian}
{
\catcode`\"=12
\unicodetable{anatolian}{"14400,"14410,"14420,"14430,"14440,"14450,"14460,"14470,"14480,"14490,"144A0,"144B0,"144C0,"144C0,"144D0,"144E0,"144F0,%
 "14500,"14510,"14520,"14530,"14540,"14550,"14560,"14570,"14580,"14590,"145A0,"145B0,"145C0,"145D0,"145E0,"145F0,%
 "14600,"14610,"14620,"14630,"14640}
}


\section{Languages and hyphenation using Lua}

\begin{texexample}{Hyphenation and Lua}{ex:luahyphen}
\begin{luacode*}
langobject = lang.new()
pattfile = io.open(kpse.find_file('hyph-el-polyton.pat.txt'), 'r')
lang.patterns(langobject, pattfile:read('*all'))
tex.print('\\language' .. lang.id(langobject))
\end{luacode*}
\end{texexample}


Javier Bezos gave a simple solution to enforcing hyphenation\footnote{\protect\url{https://tex.stackexchange.com/questions/240316/hyphenation-with-languages-in-transliteration-forced-hyphenation}}. 
Using Lua we insert a |\-| between a vowel and some consonants.
 
\begin{texexample}{Forced hyphenation}{ex:luahyphen}
\begin{luacode*}
function insdisc (s)
    -- add more letters if necessary
  s = s:gsub( [[([aeiou])([bcfgklmnpqtxz])]], '%1\\-%2' )
    -- but don't leave a single char alone:
  s = s:gsub( '^(.)\\%-', '%1')
  s = s:gsub( '\\%-(.)$', '%1')
  return s
end
\end{luacode*}

\def\insdisc#1{\directlua{tex.print(insdisc(\luastring{#1}))}}

\fbox{\parbox{1mm}{%
Text \insdisc{umiq'utzexopab} umiq'utzexopab text text text text text
text text text text text text text text text text text text.
}}
\end{texexample}













