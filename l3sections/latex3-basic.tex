\makeatletter\@specialfalse\makeatother
\parindent1em
\chapter{The Basic LaTeX3 Syntax and Approach}
 \label{ch:l3intro}
 \cxset{epigraph width=0.7\textwidth}
 \epigraph{A final hint: listen carefully to what language users say they
want, until you have an understanding of what they really want. Then
find some way of achieving the latter at a small fraction of the cost
of the former. This is the test of success in language design, and
of progress in programming methodology. Perhaps these two are the same
subject anyway.}{C.A.R. Hoare, 1973}

		
\epigraph{Frank, in case you needed encouragement, please bear this in mind: I'm very much down at the blunt end of (La)TeX -- almost a total end-user. Following an earlier recommendation in this Q\&A, I visited the expl3 manual and was scared witless... Hope you can understand that---it's not a complaint, just an indication of the intellectual/experience distance from here to there.}{---Brent.Longborough Mar 2 '12 at 9:02 at \href{https://tex.stackexchange.com/questions/45838/what-can-i-do-to-help-the-latex3-project/46427\#46427}{SX.TX}}
 
Niklaus Wirth, the developer of the Pascal language long back in the 70’s wrote a paper titled \emph{On the Design of Programming Languages}. In his paper Wirth advocated that an important aspect of language design is \emph{simplicity}. He later on described the lessons learnt from his own works as:\footnote{\protect\url{http://chrisposkitt.com/tag/wirth/}}:

\begin{enumerate}
\item Writing a program is difficult.
\item Writing a correct program is even more so.
\item Writing a publishable program is exacting.
\item Programs are not written. They grow!
\item Controlling growth needs much discipline.
\item Reducing size and complexity is the triumph.
\item Programs must not be regarded as code for computers, but as literature for humans.
\end{enumerate}

The LaTeX3 syntax can only be described with some awe as `different’, although it retains some remnants of 
\tex’s syntax retaining the backslash, it is so different that many developers and package writers have resisted its adoption irrespective of the fact that it offers some solid code. 

Resistance to the language is understandable and noticed early by Computer Science pioneers. Hoare wrote:

\cxset{quotation font-size=\normalsize}
\begin{quotation}
A necessary condition for the achievement of any of these objectives
is the utmost simplicity in the design of the language. Without simplicity,
even the language designer himself cannot evaluate the consequences of his
design decisions. Without simplicity, the compiler writer cannot achieve
even reliability, and certainly cannot construct compact, fast and
- efficient compilers. But the main beneficiary of simplicity is the user
of the language. In all spheres of human intellectual and practical
activity, from carpentry to golf, from sculpture to space travel, the
true craftsman is the one who thoroughly understands his tools. And this
applies to programmers too. A programmer who fully understands his
language can tackle more complex tasks, and complete them quicker and
more satisfactorily than if he did not. In fact, a programmer's need
for an understanding of his language is so great, that it is almost
impossible to persuade him to change to a new one. No matter what the
deficiencies of his current language, he has learned to live with them;
he has learned how to mitigate their effects by discipline and documentation,
and even to take advantage of them in ways which would be impossible
in a new and cleaner language which avoided the deficiency.

It therefore seems especially necessary in the design of a new
programming language, intended to attract programmers away from their
current high level language, to pursue the goal of simplicity to an
extreme, so that a programmer can readily learn and remember all its
features, can select the best facility for each of his purposes, can
fully understand the effects and consequences of each decision, and can
then concentrate the major part of his intellectual effort to understanding
his problem and his programs rather than his tool.
\end{quotation}

I have been programming for many years and have a disdain for languages that---as Hoare
put it--- I cannot remember ``all its features’’.  LaTeX3 has not achieved the level of simplicity required in its core. As a tool it fails the simplicity test and effortful learning is necessary to use it effectively. Currently there are probably less than twenty developers that understand it fully. 

Where, \latex3 excels is its architecture, overall plan and direction and modularizing the code to an extend that the required tools reside in logically set modules or classes in \latex’s terminology. What I can promise you, once you master it, there is no looking back. 

\section{Is it stable?}

One question that often arises is the stability of the current \latex~3 code base. Of course the degree to which software are “stable enough” depends on the requirements. Joseph Wright, answering a question on the SX.TX Q\&A site wrote:

\begin{latexquotation}
If you want 'will never change again', then plain TeX is probably your best bet. Knuth does still fix bugs periodically, but most things are now likely to be regarded as 'features' rather than bugs and so it's extremely likely that a document written in plain today will still work totally unchanged in tens of years (assuming TeX systems continue to be available).

The LaTeX2e kernel is also very unlikely to change further, and so is almost if not quite as stable as TeX itself. The team do fix bugs and do allow a bit more leeway than Knuth does, but even so it's extremely unlikely anything will change with LaTeX2e at the kernel level in a way that would require changes in documents.

There are some LaTeX packages one could reasonably decide to use which are also very stable and unlikely to see changes, either because they are no longer being actively developed or because the authors are careful to only change code related to genuine bugs or new, non-breaking, features. Obvious candidates are keyval, graphicx, etc.: probably there is actually quite a decent list, depending on your requirements.

In the case of the LaTeX3 packages l3kernel and l3packages, 'stable' does not extend as far as 'you will never have to make a change to a document using them', at least at this stage. What it means is that the team will not be making 'arbitrary' changes and will document/announce when this happens. Most of l3kernel is 'done', with the plans primarily focused on addition of new functionality rather than altering existing code. However there are a few places where we know some change may be required, and that will be announced on the LaTeX-L mailing list and documented. Even within these changes, 'breaking' (non-back-compatible) alterations will be small in number, but there is at least one of them we still need to do.

In the case of xparse, \docAuxCommand*{DeclareDocumentCommand} and so on are 'stable' in the sense that they will only be augmented, not removed, but there could be some changes on the more esoteric functions (for example, there are questions centred on the \textbf{g} argument type).

Thus 'stable enough' depends on your use case. If you can live with 'will have to make very occasional changes based on documented and scheduled updates' then expl3 is entirely usable. (I and others use if routinely in packages.) On the other hand, if you want 'this code must work with no changes with all future releases of support code' then we are not quite there yet.
\end{latexquotation}

\section{Getting started}

Other than the obvious of making sure you have the latest distribution from the LaTeX3 repository, 
the first step is to understand the conventions used by the \LaTeX3 developers. Macros are termed \meta{functions} and \meta{variables}. Macro names in general use the underscore and the colon in their names.
This is by design and to be honest is part of what many developers are unhappy about. It does cut down on the readability of the code and the longer names are more difficult to remember. This type of naming convention is similar to Hungarian notation, in which the name of a variable or function indicates its type  or its intended use and it does not have a lot of friends.


Consider the \tex primitive \docAuxCommand*{meaning}. In \latex3 it has been remapped to \docAuxCommand*{token_to_meaning:N}. Similarly \docAuxCommand*{scan_stop:} has been let to \docAuxCommand*{relax}.

\begin{texexample}{Getting started}{ex:meaning}
\ExplSyntaxOn
\def\somevar{one}
\token_to_meaning:N \scan_stop:  \\
\meaning\somevar \\
\token_to_meaning:N \somevar \\
\token_to_meaning:N \token_to_meaning:N
\ExplSyntaxOff
\end{texexample}

The part that comes after the colon is termed the \emph{function signature}. For example in |token_to_meaning:N|, the function signature is the \textbf{N}. The individual letter “N” is termed the argument specifier. Another important part is the prefix of the functions. There are some exceptions but the prefix normally indicates the module where the macro has been defined. So |\token_to_meaning:N|  can be found in the |l3token| package.\footnote{The term module and package are used interchangeably by the \latex3 Team.}

Consider the definition of a simple function  |\phd_print_xy:nn| that accepts two values $x,y$ and prints them. This can be defined by one of the |cs_| type functions.

One way we could have defined the macro using  \tex would be:

\begin{teXXX}
\def\phdprint #1#2{x#1 y#2}
\end{teXXX}

Using \latexe we would have probably used |\newcommand| and if the definition was internal to a package used an |@|. 

\begin{teXXX}
\makeatletter
\newcommand\phd@print [2] {x#1 y#2}
\makeatother
\end{teXXX}

In \latex3 we would use |\cs_set_no_par:Npn|.

\begin{teXXX}
\cs_set_nopar:Npn \phd_print_xy:nn #1#2 { x #1 y #2 }
\end{teXXX}

So what is this mysterious |\cs_set_nopar:Npn|? We can find out by peeking at its meaning. This is shown in Example~\ref{ex:somemeaning}. As you can see behind the new dress is Knuth’s same old |\def|.

\begin{texexample}{The meaning of a command}{ex:somemeaning}
\ExplSyntaxOn
\token_to_meaning:N  \cs_set_nopar:Npn
\ExplSyntaxOff
\end{texexample}

But first let us examine the |:Npn| part of the |\cs_set_no_par:Npn| more carefully. What this means is the macro has three arguments. The first one is N-type which is a \tex token. The second one is p-type, which denotes normal \tex parameters such as |#1#2|. Lastly the n-type can be either a single token or a bracketted parameter. 

There are many more argument specifiers. Functions can be found with different argument specifiers and these are termed \emph{variants}. Recall that a macro can be defined using |\def|, |\edef| or a |\csname| construct. The argument specifier to the |\cs_setnopar| can be varied to achieve it. 

\begin{texexample}{The meaning of a command}{ex:somemeaning}
\ExplSyntaxOn
\token_to_meaning:N  \cs_set_nopar:Npx\\
\token_to_meaning:N  \cs_set_nopar:cpx\\
\ExplSyntaxOff
\end{texexample}

Consider the use  of a |\csname| construct to define our |\phd_print_xy:nn| macro. The example that follows

\begin{texexample}{ex:csname}{ex:csname}

\ExplSyntaxOn
\expandafter\def\csname phd_print_xy:nn\endcsname #1 #2{x#1 y#2}

\token_to_meaning:N \phd_print_xy:nn\\

\cs_set_nopar:cpx {phd_print_xy:nn} #1 #2 {x#1 y#2}
\token_to_meaning:N \phd_print_xy:nn\\
\ExplSyntaxOff
\end{texexample} 

By using \latex3 functions, we do not need to use the |\expandafter| macro. The macros are generally longer but the overall code is shorter.

So far we have used the |\token_to_meaning:N|. \latex3 offers similar commands to get the argument specification, the prefix and the replacement specification. When we specify a macro in \latex3 we can capture all its constituent parts and handle them individually if we want.

\begin{texexample}{Dissecting a macro}{}
\ExplSyntaxOn
\cs_set_nopar:Npn \phd_print_xy:nn #1#2! { x #1 y #2 }
\token_to_meaning:N \phd_print_xy:nn \\
\token_get_arg_spec:N \phd_print_xy:nn  \\
\token_get_prefix_spec:N \phd_print_xy:nn\\
\token_get_replacement_spec:N \phd_print_xy:nn\\
\ExplSyntaxOff
\end{texexample}


Some argue that the syntax is not syntactic sugar but syntactic cyanide that changes the look and feel both of \latexe and \tex command macros. You should think of |expl3| as a new computer language. It does introduce consistency and offers a full repertoire of tools. The syntactic strangeness of the language does introduce barriers to mastering it, but the advantages far outweigh the difficulties of the language.


The eye tends to miss the argument specifier, it is important to note that the macro
name is \cmd{\test\_something:nn} and not \cmd{\test\_something} and the factory command is |\cs_new:Npn| and not |\cs_new|. If you have been programming using traditional macros this is a common mistake that you will accidentally make and you will get an |error unknown| message.

\section{Where from here}

The chapters of this book follow a logical sequence for learning the language, although most of them can be read as stand alone. 

The steps in learning any computer language require a logical sequence of study:

\begin{enumerate}
\item Understanding the syntax
\item Variables and datatypes
\item Numbers and assignments
\item Control Structures
\item Functions
\item Data structures
\item Ecosystem
\end{enumerate}

In the next chapter we would study the creation of functions in more detail. This is the most important skill to master before you proceed with the rest of the programming constructs, such as iteration, arithmetic operations etc.



\chapter{Defining Functions and Variables}

\section{Defining functions}
There are two main methods to define functions. In the first method you are required to use parameter tex, whereas in the second this can be left out, as it can be inferred from the argument specification of the function being defined. The functions used to create other functions can be found in both forms. For example:

\begin{texexample}{Using parameter text}{}
\ExplSyntaxOn
\cs_set_nopar:Npn \phd_print:n #1 {#1}
\token_to_meaning:N \phd_print:n\\

\cs_set_nopar:Nn  \phd_print:n  {#1}


\token_to_meaning:N \phd_print:n\\
\ExplSyntaxOff
\end{texexample}



 Functions can be created with no requirement that they are declared
 first (in contrast to variables, which must always be declared).\footnote{This primarily refers to variables that require a \tex register.}
 Declaring a function before setting up the code means that the name
 chosen will be checked and an error raised if it is already in use.
 The name of a function can be checked at the point of definition using
 the \docAuxCommand*{cs_new}\ldots functions: this is recommended for all
 functions which are defined for the first time.

 There are three primary ways to define new functions, using |new|, |set| or |gset| variations.  The first one is similar to the \latexe |\newcommand|, and produces macros that will generate an error if there is an attempt to redefine them. The other two are variations of the |\def or \edef| and |\gdef or \xdef| \tex commands.
 
 All classes define a function to expand to the substitution text.
 Within the substitution text the actual parameters are substituted
 for the formal parameters (|#1|, |#2|, \ldots).
 
 \begin{description}
   \item[\texttt{new}]
     Create a new function with the \texttt{new} scope,
     such as \docAuxCommand* {cs_new:Npn}.  The definition is global and will result in
     an error if it is already defined.
   \item[\texttt{set}]
     Create a new function with the \texttt{set} scope,
     such as \docAuxCommand* {cs_set:Npn}. The definition is restricted to the current
     \TeX{} group and will not result in an error if the function is already
     defined.
   \item[\texttt{gset}]
     Create a new function with the \texttt{gset} scope,
     such as \docAuxCommand* {cs_gset:Npn}. The definition is global and
     will not result in an error if the function is already defined.
 \end{description}

  Finally, the functions in
 Subsections~\ref{sec:l3basics:defining-new-function-1}~and
 \ref{sec:l3basics:defining-new-function-2} are primarily meant to define
 \emph{base functions} only. Base functions can only have the following
 argument specifiers:
 \begin{description}
   \item[|N| and |n|] No manipulation.
   \item[|T| and |F|] Functionally equivalent to |n| (you are actually
     encouraged to use the family of |\prg_new_conditional:| functions
     described in Section~\ref{sec:l3prg:new-conditional-functions}).
   \item[|p| and |w|] These are special cases.
 \end{description}



 Within each set of scope there are different ways to define a function.
 The differences depend on restrictions on the actual parameters and
 the expandability of the resulting function.
 \begin{description}
   \item[\texttt{nopar}]
      Create a new function with the \texttt{nopar} restriction,
      such as \docAuxCommand*{cs_set_nopar:Npn}. The parameter may not contain
      \docAuxCommand*{par} tokens.
   \item[\texttt{protected}]
      Create a new function with the \texttt{protected} restriction,
      such as \docAuxCommand*{cs_set_protected:Npn}. The parameter may contain
      \docAuxCommand*{par} tokens but the function will not expand within an
      \texttt{x}-type expansion.
 \end{description}
 
 
\subsection{Defining new functions using parameter text}

Theses function are \TeX ish in style, as compared to those functions that use the signature to automatically detect the number of parameters and are more \LaTeX-like. They are mainly used with the |:Npn| signature specification.

\begin{texexample}{Using parameter text}{}
\ExplSyntaxOn
\cs_new:Npn \phd_print:n #1 {#1}

\token_to_meaning:N \cs_new:Npn\\
\token_to_meaning:N \phd_print:n\\
\ExplSyntaxOff
\end{texexample}

\begin{docCommand}{cs_new:Npn} {\meta{function} \meta{parameters} \marg{code}}
Creates \meta{function} to expand to \meta{code} as replacement text. Within the \meta{code}, the
\meta{parameters} (\#1, \#2, etc.) will be replaced by those absorbed by the function. The
definition is \textbf{global} and an error will result if the \meta{function} is already defined.
Variants with |cpn,Npx,cpx| are predefined by the kernel.
\end{docCommand}

The |:Npn| form can also be used even if there is no parameter text. However this is considered a constant variable and is preferred to be coded as a |tl| such.

\begin{texexample}{Usage of the macro}{ex:csnew}
\ExplSyntaxOn
  \cs_new:Npn \copyrightfootnote: 
    {
      \footnotetext{Copyright~(2014-2015)~of~Yiannis~Lazarides,~distributed~
      under~the~\LaTeX{}~Project~Public~License~(LPPL).}
    }
  \copyrightfootnote:
\ExplSyntaxOff
\end{texexample}

An important point to note is if you use the function signature type you will get an error if the trailing |:| is not used in the macro name. 

\begin{teXXX}
\cs_new:Nn \copyrightafootnote 
  {
    ...
  }
\copyrightafootnote
\end{teXXX}

This will produce an error:\ExplSyntaxOn\copyrightfootnote:\ExplSyntaxOff

\begin{verbatim}
! LaTeX error: "kernel/missing-colon"
! Function '\copyrightafootnote' contains no ':'.
! See the LaTeX3 documentation for further information.
! For immediate help type H <return>.
\end{verbatim}

If the function is redefined, it will produce an error, similar to \latexe |\newcommand|. However, do note that the |set| family of commands can silently overwrite it. 

\begin{texexample}{Usage of the macro \protect\string\cs\_gset:Npn}{ex:csnew}
\ExplSyntaxOn
\cs_gset:Npn \copyrightfootnote: {\footnotetext{Copyright~(2014-2015)~of~Yiannis~Lazarides,~distributed~
under~the~\LaTeX{}~Project~Public~License~(LPPL).}}
\copyrightfootnote:
\ExplSyntaxOff
\end{texexample}

\begin{docCommand}{cs_new_nopar:Npn} {\meta{function} \meta{parameters} \marg{code}}
Creates \meta{function} to expand to \meta{code} as replacement text. Within the \meta{code}, the
\meta{parameters} (\#1, \#2, etc.) will be replaced by those absorbed by the function. When the
\meta{function} is used the hparametersi absorbed cannot contain \par tokens. The definition
is global and an error will result if the \meta{function} is already defined.
\end{docCommand}

\begin{texexample}{Meaning}{}
\ExplSyntaxOn
\token_to_meaning:N \cs_new_nopar:Npn
\ExplSyntaxOff
\end{texexample}

\begin{docCommand}{cs_new_protected:Npn}{\meta{function} \meta{parameters} \marg{code}}
Creates \meta{function} to expand to \meta{code} as replacement text. Within the hcodei, the
hparametersi (\#1, \#2, etc.) will be replaced by those absorbed by the function. The
\meta{function} will not expand within an x-type argument. The definition is global and an
error will result if the hfunctioni is already defined.
\end{docCommand}

\begin{docCommand}{cs_new_protected_nopar:Npn}{\meta{function} \meta{parameters} \marg{code}}
Creates \meta{function} to expand to \meta{code} as replacement text. 
When the \meta{function} is used the \meta{parameters} absorbed cannot contain \docAuxCommand*{par} tokens. The hfunctioni
will not expand within an x-type argument. The definition is global and an error will
result if the \meta{function} is already defined.
\end{docCommand}

This brings us to the end of the |new| type functions that can be used for function definitions. They all have variants of the form |cpn| and |cpx| and the base function for edef also is available. You can consult the manual for more definitions.

\subsubsection{The set type functions}

The rest of the commands are variations using the |set| form of function creating macros. These do not issue a 
warning if redefined.

 \begin{docCommand}{cs_set:Npn} {\meta{function} \meta{parameters} \marg{code}}
   Sets \meta{function} to expand to \meta{code} as replacement text.
   Within the \meta{code}, the \meta{parameters} (|#1|, |#2|,
   \emph{etc.}) will be replaced by those absorbed by the function.
   The assignment of a meaning to the \meta{function} is restricted to
   the current \TeX{} group level.
\end{docCommand}

\begin{texexample}{Meaning}{}
\ExplSyntaxOn
\token_to_meaning:N \cs_set:Npn
\ExplSyntaxOff
\end{texexample}

As can be seen from the example this is |\protected \long \def|. The |\cs_set_nopar:Npn| in the maeaning in the example is described next and is simply an equivalent function to |\def|.

 \begin{docCommand} {cs_set_nopar:Npn}{\meta{function} \meta{parameters} \marg{code}}
   Sets \meta{function} to expand to \meta{code} as replacement text.
   Within the \meta{code}, the \meta{parameters} (|#1|, |#2|,
   \emph{etc.}) will be replaced by those absorbed by the function.
   When the \meta{function} is used the \meta{parameters} absorbed
   cannot contain \cs{par} tokens. The assignment of a meaning
   to the \meta{function} is restricted to the current \TeX{} group
   level.
 \end{docCommand}
 
 \begin{texexample}{Meaning \textbackslash cs\_set\_nopar:Npn}{}
\ExplSyntaxOn
\token_to_meaning:N \cs_set_nopar:Npn
\ExplSyntaxOff
\end{texexample}
 

\begin{docCommand}{cs_set_protected:Npn} {\meta{function} \meta{parameters} \marg{code}}
   Sets \meta{function} to expand to \meta{code} as replacement text.
   Within the \meta{code}, the \meta{parameters} (|#1|, |#2|,
   \emph{etc.}) will be replaced by those absorbed by the function.
   The assignment of a meaning to the \meta{function} is restricted to
   the current \TeX{} group level. The \meta{function} will
   not expand within an \texttt{x}-type argument.
 \end{docCommand}
 \begin{texexample}{Meaning \textbackslash cs\_set\_protected:Npn}{}
 \ExplSyntaxOn
 \token_to_meaning:N \cs_set_protected:Npn
\ExplSyntaxOff
\end{texexample}
 


\begin{docCommand}{cs_set_protected_nopar:Npn}{\meta{function} \meta{parameters} \marg{code}}
   Sets \meta{function} to expand to \meta{code} as replacement text.
   Within the \meta{code}, the \meta{parameters} (|#1|, |#2|,
   \emph{etc.}) will be replaced by those absorbed by the function.
   When the \meta{function} is used the \meta{parameters} absorbed
   cannot contain \cs{par} tokens. The assignment of a meaning
   to the \meta{function} is restricted to the current \TeX{} group
   level. The \meta{function} will not expand within an
   \texttt{x}-type argument.
\end{docCommand}
\begin{texexample}{Meaning \textbackslash cs\_set\_protected\_nopar:Npn}{}
\ExplSyntaxOn
\token_to_meaning:N \cs_set_protected_nopar:Npn
\ExplSyntaxOff
\end{texexample}
 
Next the above are made available by the \latex3 kernel but all in the |global| form of the command. The syntax is identical except they use |cs_gset|.


\begin{docCommand} {cs_gset:Npn}{\meta{function} \meta{parameters} \marg{code}}
   Globally sets \meta{function} to expand to \meta{code} as replacement
   text. Within the \meta{code}, the \meta{parameters} (|#1|, |#2|,
  \emph{etc.}) will be replaced by those absorbed by the function.
  The assignment of a meaning to the \meta{function} is \emph{not}
   restricted to the current \TeX{} group level: the assignment is
   global.
\end{docCommand}
\begin{texexample}{Meaning \textbackslash cs\_gset:Npn}{}
\ExplSyntaxOn
\token_to_meaning:N \cs_gset:Npn
\ExplSyntaxOff
\end{texexample}

\begin{docCommand}{cs_gset_nopar:Npn} {\meta{function} \meta{parameters} \marg{code}}
   Globally sets \meta{function} to expand to \meta{code} as replacement
   text. Within the \meta{code}, the \meta{parameters} (|#1|, |#2|,
   \emph{etc.}) will be replaced by those absorbed by the function.
   When the \meta{function} is used the \meta{parameters} absorbed
   cannot contain \cs{par} tokens. The assignment of a meaning to the
   \meta{function} is \emph{not} restricted to the current \TeX{}
   group level: the assignment is global.
\end{docCommand}
\begin{texexample}{Meaning \textbackslash cs\_gset\_nopar:Npn}{}
\ExplSyntaxOn
\token_to_meaning:N \cs_gset_nopar:Npn
\ExplSyntaxOff
\end{texexample}


\begin{docCommand} {cs_gset_protected:Npn} {\meta{function} \meta{parameters} \marg{code}}
   Globally sets \meta{function} to expand to \meta{code} as replacement
   text. Within the \meta{code}, the \meta{parameters} (|#1|, |#2|,
   \emph{etc.}) will be replaced by those absorbed by the function.
   The assignment of a meaning to the \meta{function} is \emph{not}
   restricted to the current \TeX{} group level: the assignment is
   global. The \meta{function} will not expand within an
   \texttt{x}-type argument.
\end{docCommand}
\begin{texexample}{Meaning \textbackslash cs\_gset\_protected:Npn}{}
\ExplSyntaxOn
\token_to_meaning:N \cs_gset_protected:Npn
\ExplSyntaxOff
\end{texexample}

\begin{docCommand}{cs_gset_protected_nopar:Npn} {\meta{function} \meta{parameters} \marg{code}}
   Globally sets \meta{function} to expand to \meta{code} as replacement
   text. Within the \meta{code}, the \meta{parameters} (|#1|, |#2|,
   \emph{etc.}) will be replaced by those absorbed by the function.
   When the \meta{function} is used the \meta{parameters} absorbed
   cannot contain \cs{par} tokens. The assignment of a meaning to the
   \meta{function} is \emph{not} restricted to the current \TeX{}
   group level: the assignment is global. The \meta{function} will
   not expand within an \texttt{x}-type argument.
\end{docCommand}
\begin{texexample}{Meaning \textbackslash cs\_gset\_protected\_nopar:Npn}{}
\ExplSyntaxOn
\token_to_meaning:N \cs_gset_protected_nopar:Npn
\ExplSyntaxOff
\end{texexample}

This brings us to the end of the functions available to the developer for defining macros. It’s a lot of them. In the next section some more functions are defined, this time using the signature of the function the function are created automatically without the need to type in the parameter text.


\subsection{Defining new functions using the signature}

The functions outlined below have a simpler form in that they create other commands without the need to specify their arguments. The number of parameters is detected automatically from the function signature. Which method is the best is obvious up to the user preferences.\footnote{See discussion at SX.TX \protect{\url{http://tex.stackexchange.com/questions/240675/differences-in-latex3-function-generation-methods}}} 


\begin{docCommand}{cs_new:Nn}{\meta{function}\marg{code}}
Creates \meta{function} to expand to \meta{code} as replacement text. A nice feature is that within the \meta{code}
the number of parameters is detected automatically from the function signature. These \meta{parameters} (\#1, \#2, etc.) will be replaced by those absorbed by the function. The definition is global and an error will result if the \meta{function} is already defined.\footnote{The definitions of the commands have been taken mostly verbatim from the documentation of the package.}


\begin{texexample}{Signature}{ex:signature}
\ExplSyntaxOn
\cs_new:Nn \exampleone:nn {}
\cs_new:Nn \exampletwo:nn{#1 #2}
\exampleone:nn {one}{two}

\exampletwo:nn{one }{two}

\texttt\textbackslash\cs_to_str:N\exampleone:nn
\ExplSyntaxOff
\end{texexample}
\end{docCommand}

 
 
 
\begin{docCommand}{cs_new_nopar:Nn}{\meta{function} \marg{code}}
   Creates \meta{function} to expand to \meta{code} as replacement text.
   Within the \meta{code}, the number of \meta{parameters} is detected
   automatically from the function signature. These \meta{parameters}
   (|#1|, |#2|, \emph{etc.}) will be replaced by those absorbed by the
   function.  When the \meta{function} is used the \meta{parameters}
   absorbed cannot contain \docAuxCommand*{par} tokens. The definition is global and
   an error will result if the \meta{function} is already defined.
 \end{docCommand}

\begin{docCommand}{cs_new_protected:Nn}{\meta{function} \marg{code}}
   Creates \meta{function} to expand to \meta{code} as replacement text.
   Within the \meta{code}, the number of \meta{parameters} is detected
   automatically from the function signature. These \meta{parameters}
   (|#1|, |#2|, \emph{etc.}) will be replaced by those absorbed by the
   function. The \meta{function} will not expand within an \texttt{x}-type
   argument. The definition is global and
   an error will result if the \meta{function} is already defined.
\end{docCommand}


%
% \begin{function}
%   {
%     \docAuxCommand*_new_protected_nopar:Nn, \docAuxCommand*_new_protected_nopar:cn,
%     \docAuxCommand*_new_protected_nopar:Nx, \docAuxCommand*_new_protected_nopar:cx
%   }
%   \begin{syntax}
%     \docAuxCommand*{cs_new_protected_nopar:Nn} \meta{function} \Arg{code}
%   \end{syntax}
%   Creates \meta{function} to expand to \meta{code} as replacement text.
%   Within the \meta{code}, the number of \meta{parameters} is detected
%   automatically from the function signature. These \meta{parameters}
%   (|#1|, |#2|, \emph{etc.}) will be replaced by those absorbed by the
%   function.  When the \meta{function} is used the \meta{parameters}
%   absorbed cannot contain \docAuxCommand*{par} tokens. The \meta{function} will not
%   expand within an \texttt{x}-type argument. The definition is global and
%   an error will result if the \meta{function} is already defined.
% \end{function}

Similarly to the |cs_new| commands the |cs_set| functions create other commands, this time
with a local scope. This pattern is followed right through the kernel.

 \begin{docCommand}{cs_set:Nn}{\meta{function}\marg{code}}
   Sets \meta{function} to expand to \meta{code} as replacement text.
   Within the \meta{code}, the number of \meta{parameters} is detected
   automatically from the function signature. These \meta{parameters}
   (|#1|, |#2|, \emph{etc.}) will be replaced by those absorbed by the
   function.
   The assignment of a meaning to the \meta{function} is restricted to
   the current \TeX{} group level.
 \end{docCommand}

\begin{docCommand}{cs_set_nopar:Nn}{\meta{function}\marg{code}}
   Sets \meta{function} to expand to \meta{code} as replacement text.
   Within the \meta{code}, the number of \meta{parameters} is detected
   automatically from the function signature. These \meta{parameters}
   (|#1|, |#2|, \emph{etc.}) will be replaced by those absorbed by the
   function.  When the \meta{function} is used the \meta{parameters}
   absorbed cannot contain \docAuxCommand*{par} tokens.
   The assignment of a meaning to the \meta{function} is restricted to
   the current \TeX{} group level. This is the \tex primitive \docAuxCommand*{def}
\end{docCommand}

\begin{teXXX}
\tex_let:D \cs_set_nopar:Npn \tex_def:D
748 \tex_let:D \cs_set_nopar:Npx \tex_edef:D
749 \etex_protected:D \cs_set_nopar:Npn \cs_set:Npn
750                     { \tex_long:D \cs_set_nopar:Npn }
751 \etex_protected:D \cs_set_nopar:Npn \cs_set:Npx
752                   { \tex_long:D \cs_set_nopar:Npx }
753 \etex_protected:D \cs_set_nopar:Npn \cs_set_protected_nopar:Npn
754 { \etex_protected:D \cs_set_nopar:Npn }
755 \etex_protected:D \cs_set_nopar:Npn \cs_set_protected_nopar:Npx
756 { \etex_protected:D \cs_set_nopar:Npx }
757 \cs_set_protected_nopar:Npn \cs_set_protected:Npn
758 { \etex_protected:D \tex_long:D \cs_set_nopar:Npn }
759 \cs_set_protected_nopar:Npn \cs_set_protected:Npx
760 { \etex_protected:D \tex_long:D \cs_set_nopar:Npx }
\end{teXXX}
\ExplSyntaxOn
\meaning\cs_new:Npn
\ExplSyntaxOff


\begin{docCommand}{cs_set_protected:Nn}{\meta{function}\marg{code}}
   Sets \meta{function} to expand to \meta{code} as replacement text.
   Within the \meta{code}, the number of \meta{parameters} is detected
   automatically from the function signature. These \meta{parameters}
   (|#1|, |#2|, \emph{etc.}) will be replaced by those absorbed by the
   function. The \meta{function} will not expand within an \texttt{x}-type
   argument.
   The assignment of a meaning to the \meta{function} is restricted to
   the current \TeX{} group level.
 \end{docCommand}

\begin{docCommand}{cs_set_protected_nopar:Nn}{ \meta{function} \marg{code}}
   Sets \meta{function} to expand to \meta{code} as replacement text.
   Within the \meta{code}, the number of \meta{parameters} is detected
   automatically from the function signature. These \meta{parameters}
   (|#1|, |#2|, \emph{etc.}) will be replaced by those absorbed by the
   function.  When the \meta{function} is used the \meta{parameters}
   absorbed cannot contain \docAuxCommand*{par} tokens. The \meta{function} will not
   expand within an \texttt{x}-type argument.
   The assignment of a meaning to the \meta{function} is restricted to
   the current \TeX{} group level.
 \end{docCommand}

The next commands create functions with global scope.

 \begin{docCommand}{cs_gset:Nn}{ \meta{function} \marg{code}}
   Sets \meta{function} to expand to \meta{code} as replacement text.
   Within the \meta{code}, the number of \meta{parameters} is detected
   automatically from the function signature. These \meta{parameters}
   (|#1|, |#2|, \emph{etc.}) will be replaced by those absorbed by the
   function.
   The assignment of a meaning to the \meta{function} is  global.
 \end{docCommand}

 \begin{docCommand}{cs_gset_nopar:Nn}{ \meta{function} \marg{code}}
   Sets \meta{function} to expand to \meta{code} as replacement text.
   Within the \meta{code}, the number of \meta{parameters} is detected
   automatically from the function signature. These \meta{parameters}
   (|#1|, |#2|, \emph{etc.}) will be replaced by those absorbed by the
   function.  When the \meta{function} is used the \meta{parameters}
   absorbed cannot contain \docAuxCommand*{par} tokens.
   The assignment of a meaning to the \meta{function} is global.
 \end{docCommand}
 

\section{Copying control sequences}

Control sequences (not just functions as defined above) can be set to have the same
meaning using the functions described here. Making two control sequences equivalent
means that the second control sequence is a copy of the first (rather than a pointer to
it). Thus the old and new control sequence are not tied together: changes to one are not
reflected in the other. These are syntactic replacements for |\let|.

\begin{texexample}{Let}{}
\ExplSyntaxOn
\cs_set_nopar:Nn \testa: {AAA}
\cs_set_eq:NN\testb: \testa:
\token_to_meaning:N \testa:  \\
\cs_set_nopar:Nn \testa: {BBBB}
\testb:  \\
\token_to_meaning:N \testb:  \\
\token_to_meaning:N \testa:  \\
\testa:\\
\testb: \\
\meaning\cs_set_eq:NN

% check if equal to \let
\token_to_meaning:N \let\\
\token_to_meaning:N \cs_set_equal:NN
\ExplSyntaxOff
\end{texexample}

 In the following text \enquote{cs} is used as an abbreviation for
 \enquote{control sequence}.

 \begin{docCommand}{cs_new_eq:NN} {\meta{cs1} \meta{cs2}}
   Globally creates \meta{control sequence 1} and sets it to have the same
   meaning as \meta{control sequence 2} or |<token>|.
   The second control sequence may
   subsequently be altered without affecting the copy.
\end{docCommand}


\begin{docCommand}{cs_set_eq:NN} {\meta{cs1} \meta{cs2}}
   Sets \meta{control sequence1} to have the same meaning as
   \meta{control sequence2} (or |<token>|).
   The second control sequence may subsequently be
   altered without affecting the copy. The assignment of a meaning
   to the \meta{control sequence1} is restricted to the current
   \TeX{} group level.
 \end{docCommand}


\begin{docCommand} {cs_gset_eq:NN} {\meta{cs1} \meta{cs2}}
   Globally sets \meta{control sequence1} to have the same meaning as
   \meta{control sequence2} (or |<token>|).
   The second control sequence may subsequently be
   altered without affecting the copy. The assignment of a meaning to
   the \meta{control sequence1} is \emph{not} restricted to the current
   \TeX{} group level: the assignment is global.
\end{docCommand}

\section{Undefining control sequences}

There are occasions where control sequences need to be deleted. This is handled in a
very simple manner by the use of 
|\cs_undefine:N| \meta{control sequence},
which sets \meta{control sequence} to be globally |undefined|.

\begin{texexample}{Undefining control sequences}{ex:undefine}
\ExplSyntaxOn
\cs_set_nopar:Npn \testa: {AAA}
\cs_set_nopar:cpn {testb} {AAA}

\cs_undefine:N \testa:
\cs_undefine:c {testb}
\token_to_meaning:N \cs_undefine:N\\

\token_to_meaning:N \testa:\\
\token_to_meaning:c {testb}\\
\token_to_meaning:N \token_to_meaning:c
\ExplSyntaxOff
\end{texexample}

The function would simply set the command to the \tex primitive |undefine|, as can be seen from the example.
There is another group of commands associated with constructor functions.

\section{Converting to and from control sequences}

\begin{docCommand}{cs_if_exist_use:N} {\meta{control sequence}}
Tests whether the \meta{control sequence} is currently defined (whether as a function or another
control sequence type), and if it does inserts the \meta{control sequence} into the input stream.
\end{docCommand}

\begin{docCommand}{cs_if_exist_use:NTF} {\meta{control sequence}}
Tests whether the \meta{control sequence} is currently defined (whether as a function or another
control sequence type), and if it does inserts the \meta{control sequence} into the input stream
followed by the \meta{true code}.
\end{docCommand}

\begin{texexample}{Converting to and from control sequences}{ex:ifexists}
\ExplSyntaxOn
\cs_if_exist_use:NTF \test {}{\FALSE}
\ExplSyntaxOff
\end{texexample}

Note that numerous times, I have typed |\cs_if_exists_use:NTF| rather than the more grammatical  |\cs_if_exist_use:NTF| with consequent errors. Grammar is hardwired in the brain and it requires mental effort to write ungrammatical commands. This is an issue that needs to be addressed by the \latex3 developers. 

The famous |\csname| is mapped in this section of the module as well. Unpredictably, it got a shorter name, but a weird suffix |w|! It deserves both as it is the workhorse of \tex. The remapped commands are formally described in the manual as shown below:

\begin{docCommand}{cs:w} {\meta{control sequence name} \texttt{cs\_end:}}
Converts the given \meta{control sequence name} into a single control sequence token. This
process requires one expansion. The content for \meta{control sequence name} may be literal
material or from other expandable functions. The \meta{control sequence name} must, when
fully expanded, consist of character tokens which are not active: typically, they will be
of category code 10 (space), 11 (letter) or 12 (other), or a mixture of these.
\end{docCommand}


\section{User Commands}

All the commands above are at the programming level. For the development of user commands the \pkgname{xparse} package provides some extremely useful commands. These are dealt under \nameref{ch:xparse}
on page \pageref{ch:xparse}.

\begin{teXXX}
\NewDocumentCommand{\kant}{s>{\SplitArgument{1}{-}}O{1-7}}
  {
   \group_begin:
   \IfBooleanTF{#1} (*@\label{starargument}@*)
     { \cs_set_eq:NN \kgl_par: \kgl_star: }
     { \cs_set_eq:NN \kgl_par: \kgl_nostar: }
     \kgl_process:nn #2
    \kgl_print:
   \group_end:
  }
\end{teXXX}

In Line~\ref{starargument} we test for the star version of the command and then we continue examining the optional argument |O{1-7}|, but first and here is the magic, we have passed the argument through a pre-processing macro named |\SplitArgument|, which has captured the splitted argument and placed it, into two braced macros. It then passes it to a second macro |\getwords| that expects two mandatory aruguments and which handles the typesetting of the two words.
    
\begin{texexample}{Split Argument}{}    
\NewDocumentCommand{\separatewords}{>{\SplitArgument{1}{-}}m}{\getwords#1}
\NewDocumentCommand{\getwords}{ m m }{First word:#1  Second~Word:#2}
\separatewords{mail-coach}

\separatewords{mail}

\end{texexample}    

A similar example see TX.SX.\footnote{\protect{\url{http://tex.stackexchange.com/questions/154941/new-command-in-tex-for-fraction/154950\#154950}}}


\chapter{LaTeX3 Control Structures}
 \section{The boolean data type}

 This section describes a boolean data type which is closely
 connected to conditional processing as sometimes you want to
 execute some code depending on the value of a switch
 (\emph{e.g.},~draft/final) and other times you perhaps want to use it as a
 predicate function in an |if_predicate:w| test. The problem of the
 primitive \docAuxCommand*{if_false:} and \docAuxCommand*{if_true:} tokens is that it is not
 always safe to pass them around as they may interfere with scanning
 for termination of primitive conditional processing. In \latex3
 two canonical booleans ar employed: \docAuxCommand*{c_true_bool} or
\docAuxCommand{c_false_bool}. Besides preventing problems as described above. This also let
to the implementation of  a simple boolean parser supporting the
 logical operations And, Or, Not, \emph{etc.}\ which can then be used on
 both the boolean type and predicate functions.

 All conditional |\bool_| functions except assignments are expandable
 and expect the input to also be fully expandable (which will generally
 mean being constructed from predicate functions, possibly nested).
 
Before a boolean can be used it needs to be created with \docAuxCommand{bool_new:N}, but first let us make sure we understand what a boolean is. A Boolean data type is a data type, having two values (usually denoted \emph{true} and \emph{false}), intended to represent the truth values of logic and Boolean algebra. It is named after George Boole, who first defined an algebraic system of logic in the mid 19th century. 

So how does \latex3 construct a boolean? If we examine the code, which we will in a small example, we can see that a boolean variable is just another macro that either stores 0 or 1. If the value is odd then the boolean is \emph{true} else the boolean is \emph{false}. 

\begin{teXXX}
\tex_chardef:D \c_true_bool = 1 ~
\tex_chardef:D \c_false_bool = 0 ~
\end{teXXX}

\begin{teXXX}
 \cs_new_protected:Npn \bool_new:N #1 { \cs_new_eq:NN #1 \c_false_bool }
 \cs_generate_variant:Nn \bool_new:N { c }
\end{teXXX}

When a new boolean is constructed it is always set to false, as is evident from its code. 

Here is the formal syntax of the |\bool_new:N| function.

 \begin{docCommand}{bool_new:N}{\meta{boolean}}
   Creates a new \meta{boolean} or raises an error if the
   name is already taken. The declaration is global. The
   \meta{boolean} will initially be \texttt{false}. Once the boolean is created
   it can be set to logical true or false using \docAuxCommand*{bool_set_false:N} and \docAuxCommand*{bool_set_true:N}.
 \end{docCommand}
 
\begin{texexample}{Booleans}{}
\ExplSyntaxOn
\bool_new:N \mybool
\bool_set_false:N \mybool
\bool_if:NTF\mybool { \PASS } { \FAIL }
\ExplSyntaxOff
\end{texexample}
 

 
The real strength of the \latex~3 macros are the convenience of providing for |Or| and |And|
operations, negation etc.  and for its ability to evaluate fully boolean expressions. 

\begin{docCommand}{bool_if:nTF}{\marg{boolean expression} \marg{true code} \marg{false code}}
   Tests the current truth of \meta{boolean expression}, and
   continues expansion based on this result. The
   \meta{boolean expression} should consist of a series of predicates
   or boolean variables with the logical relationship between these
   defined using |&&| (\enquote{And}), \verb"||" (\enquote{Or}),
   |!| (\enquote{Not}) and parentheses. Minimal evaluation is used
   in the processing, so that once a result is defined there is
   not further expansion of the tests. 
\end{docCommand}   



\begin{texexample}{Booleans}{}
\ExplSyntaxOn
\bool_new:N\chapterfloat
\bool_new:N\numberfloat
\bool_set_false:N\chapterfloat
\bool_set_true:N\numberfloat

\bool_if:nTF {\chapterfloat || \numberfloat}  { \TRUE }{ \FALSE }

\bool_if:nTF {\chapterfloat && \numberfloat}  { \TRUE }{ \FALSE }

\ExplSyntaxOff
\end{texexample}

\subsection{\textbackslash if\_meaning}

The primitive |ifx| conditional has an equivalent in \latex3. This is called more semantically \docAuxCommand*{if_meaning:w}. This compares two tokens based on their meaning.



\begin{texexample}{Test ifx}{}
\ExplSyntaxOn
\group_begin:
  \cs_set_nopar:Npn \a: {BBB}
  \cs_set_nopar:Npn \b: {BBB~}
  \cs_set_nopar:Npn \c: {B~BB}
  
  \if_meaning:w \a:\b: \PASS \else: \FAIL \fi:
  \if_meaning:w \a:\c: \PASS \else: \FAIL \fi:
  
  \token_to_meaning:N \b:\\
  \token_to_meaning:N \a:  
\group_end:  
\ExplSyntaxOff
\end{texexample}

\begin{texexample}{LaTeX2e booleans}{}
\makeatletter
\ExplSyntaxOn
\if@mainmatter
     in~main~text
   \else
    not~in~main~text  
\fi    

 \meaning\@mainmattertrue\\
\bool_new:N \phd_mainmatter_bool 
\meaning\phd_mainmatter_bool
\ExplSyntaxOff
\makeatother  
\end{texexample}

\section{Predicate functions}

Predicate functions are one of the more powerful features of |expl3|. What are predicate functions? They are macros that test a predicate (\emph{true} or \meta{false}) and branch to either a true or false branch or just a single branch depending on the signature of the function. The |expl3| package has numerous such functions for example:

\begin{teXXX}
 \str_if_eq:nnT {}{}{}
\end{teXXX}

accepts two strings and if true does something. The expl3 package, provides a function that can generate such predicate functions fairly easily.

\begin{docCommand}{prg_set_conditional:Npnn}{\meta {function name}: \meta{arg spec} \meta{parameters} \marg{conditions code}}

These functions create a family of conditionals using the same \meta{code} to perform the
test created. Those conditionals are expandable if \meta{code} is. The new versions will
check for existing definitions and perform assignments globally (cf. |\cs_new:Npn|) whereas
the set versions do no check and perform assignments locally (cf. |\cs_set:Npn|). The
conditionals created are dependent on the comma-separated list of \meta{conditions}, which
should be one or more of p, T, F and TF.
\end{docCommand}

\begin{teXXX}
\prg_set_conditional:Npnn \cs_if_exist:N #1 { p , T , F , TF }
 {
 \if_meaning:w #1 \scan_stop:
   \prg_return_false:
     \else:
        \if_cs_exist:N #1
           \prg_return_true:
        \else:
          \prg_return_false:
      \fi:
 \fi:
}
2556 \prg_new_conditional:Npnn \token_if_eq_meaning:NN #1#2 { p , T , F , TF }
2557 {
2558 \if_meaning:w #1 #2
2559 \prg_return_true: \else: \prg_return_false: \fi:
2560 }

2201 \prg_new_conditional:Npnn \mode_if_math: { p , T , F , TF }
2202 { \if_mode_math: \prg_return_true: \else: \prg_return_false: \fi: }

\prg_new_conditional:Npnn \int_if_even:n #1 { p , T , F , TF}
3321 {
3322 \if_int_odd:w \__int_eval:w #1 \__int_eval_end:
3323 \prg_return_false:
3324 \else:
3325 \prg_return_true:
3326 \fi:
3327 }
\end{teXXX}

\begin{texexample}{isEven}{}
\ExplSyntaxOn
\prg_new_conditional:Npnn \isEven:n #1 { p, T, F, TF}
{
 \if_int_odd:w \__int_eval:w #1 \__int_eval_end:
    \prg_return_false:
 \else:
    \prg_return_true:
 \fi:
}

\isEven:nTF {2045679}{\PASS}{\FAIL}
\isEven:nTF {1000000}{\PASS}{\FAIL}
\ExplSyntaxOff
\end{texexample}

A common need for programmers is the testing of an integer or real for positiveness  with expl3 we can use predicate functions. In Example~\ref{ex:positive} we define predicate functions \docAuxCommand*{isPositive:nTF} to test an integer expression and feed the results to a true or false branch or according to the function signature. 

\begin{texexample}{isPositive} {ex:positive}
\ExplSyntaxOn
\prg_new_conditional:Npnn \isPositive:n #1 { p, T, F, TF}
{
\if_int_compare:w  \__int_eval:w #1 \__int_eval_end: >\__int_eval:w 0 \__int_eval_end:
     \prg_return_true:
\else:
    \prg_return_false:
\fi:          
}

\prg_new_conditional:Npnn \isNegative:n #1 { p, T, F, TF}
{
\if_int_compare:w  \__int_eval:w #1 \__int_eval_end: >\__int_eval:w 0 \__int_eval_end:
     \prg_return_false:
\else:
    \prg_return_true:
\fi:          
}
\cs_new:Npn \assert_is_positive:n #1 
   {
     \isPositive:nTF {#1} {\PASS #1} {\FAIL #1}
   }  
\cs_new:Npn \assert_is_negative:n #1 
   {
     \isNegative:nTF {#1} {\PASS #1} {\FAIL #1}
   } 
\assert_is_positive:n {2059+23-1245}
\assert_is_positive:n {-2059+23-1245}
\assert_is_negative:n {2059+23-1245}
\assert_is_negative:n {-2059+23-1245}
\ExplSyntaxOff
\end{texexample}

In the next example  we will use a common \tex trick to determine if a number is an integer or not. When \tex tries to convert a number to roman it will not scan past a minus sign .

\begin{texexample}{isInteger} {ex:isinteger}
\ExplSyntaxOn
\prg_set_conditional:Npnn \isInteger:n #1 { p, T, F, TF}
{
   \tl_if_blank:oTF {#1}{\prg_return_false:}
    {
     \tl_if_blank:oTF {  \__int_to_roman:w -\__int_eval:w #1 \__int_eval_end: }
		   {
		     \prg_return_true:
		   }
		   {
		     % not a number, but can be a negative number
		     \prg_return_false:
	         }
   }   
}

\cs_new:Npn \assert_is_integer:n #1 
   {
     \isInteger:nTF {#1} {\PASS\ ~~ #1} {\FAIL\ ~~ #1}\par
   }  
\assert_is_integer:n { }   
\assert_is_integer:n { 12}
\assert_is_integer:n {2059+1}
\assert_is_integer:n {-2059}
\assert_is_integer:n {2059}
%\assert_is_integer:n {ABC-1245}
\ExplSyntaxOff
\end{texexample}

The tests will pass provided even if we pass a  |numexpr|, but the assertion will fails if the number is negative. 
What we should have done was to test first if the head of the string was a (-) and then send it for further processing. 

\begin{texexample}{Testing the head of a string for the minus sign}{ex:string}
\ExplSyntaxOn
\cs_set:Npn \test:#1#2;{
   \str_if_eq:nnTF {-}{#1}{\PASS\par }{\FAIL\par }
   \str_if_eq:nnTF {-}{#1#2}{\PASS\par }{\FAIL\par }
}
\test:-;
\test:-12356;
\test:1234;
\ExplSyntaxOff
\end{texexample}

This passes all the comparison correctly, so we will have to re-write our function to test for the minus sign, before we send it to the main function. The reason I wrote the two tests above, is that a minus sign cannot be considered a number. 




\chapter{LaTeX3 String Manipulation and other Goodies}

 \TeX{} associates each character with a category code: as such, there is no
 concept of a \enquote{string} as commonly understood in many other
 programming languages. However, there are places where we wish to manipulate
 token lists while in some sense \enquote{ignoring} category codes: this is
 done by treating token lists as strings in a \TeX{} sense.

 A \TeX{} string (and thus an \pkg{expl3} string) is a series of characters
 which have category code $12$ (\enquote{other}) with the exception of
 space characters which have category code $10$ (\enquote{space}). Thus
 at a technical level, a \TeX{} string is a token list with the appropriate
 category codes. In this documentation, these will simply be referred to as
 strings: note that they can be stored in token lists as normal.

 The functions documented here take literal token lists,
 convert to strings and then carry out manipulations. Thus they may
 informally be described as \enquote{ignoring} category code. Note that
 the functions \docAuxCommand*{cs_to_str:N}, \docAuxCommand*{tl_to_str:n}, \docAuxCommand*{tl_to_str:N} and
 \docAuxCommand*{token_to_str:N} (and variants) will generate strings from the appropriate
 input: these are documented in \pkg{l3basics}, \pkg{l3tl} and \pkg{l3token},
 respectively.

 \section{The first character from a string}

 \begin{docCommand}{str_head:n}{\docAuxCommand*{str_head:n} \marg{token list}}
   Converts the \meta{token list} into a string, as described for
   \docAuxCommand*{tl_to_str:n}. The \docAuxCommand*{str_head:n} function then leaves
   the first character of this string in the input stream.
   The \docAuxCommand*{str_tail:n} function leaves all characters except
   the first in the input stream. The first character may be
   a space. If the \meta{token list} argument is entirely empty,
   nothing is left in the input stream.
 \end{docCommand}

\begin{texexample}{Strings}{ex:strings}
\ExplSyntaxOn
\DeclareDocumentCommand\asentence{ m }{
  \str_head:n {#1}\par}
  
\asentence{This is something}  

\str_head:n{\This~is~something}\par
\str_tail:n{\This~is~something}
\ExplSyntaxOff


\end{texexample}

 \subsection{Tests on strings}

The package provides some very powerful commands that can be used in string comparisons. Internally the comparisons are carried out using |\pdfstrcmp|. This has some complications in LuaTeX. 

 \begin{docCommand}{str_if_eq_x:nnTF}{\docAuxCommand*{str_if_eq_p:nn} \marg{tl1} \marg{tl2}}
%     \docAuxCommand*{str_if_eq:nnTF} \Arg{tl_1} \Arg{tl_2} \Arg{true code} \Arg{false code}
%   \end{syntax}
   Compares the two \meta{token lists} on a character by character
   basis, and is \texttt{true} if the two lists contain the same
   characters in the same order. Thus for example
   \begin{verbatim}
     \str_if_eq_p:no { abc } { \tl_to_str:n { abc } }
   \end{verbatim}
   is logically \texttt{true}.
\end{docCommand}


\begin{texexample}{String comparisons}{ex:test}
\ExplSyntaxOn
\let\abc\empty
\str_if_eq_x:nnTF{abc}{abc}{\TRUE}{\FALSE}\par
\str_if_eq_x:nnTF{\abc}{\abc}{\TRUE}{\FALSE}
\ExplSyntaxOff
\end{texexample}

 \section{String manipulation}

 \begin{docCommand}{str_lower_case:n}{\marg{tokens}}
%      \str_lower_case:n, \str_lower_case:f, 
%      \str_upper_case:n, \str_upper_case:f
%   }
%   \begin{syntax}
%     \docAuxCommand*{str_lower_case:n} \Arg{tokens}
%     \docAuxCommand*{str_upper_case:n} \Arg{tokens}
%   \end{syntax}
   Converts the input \meta{tokens} to their string representation, as
   described for \docAuxCommand*{tl_to_str:n}, and then to the lower or upper
   case representation using a one-to-one mapping as described by the
   Unicode Consortium file |UnicodeData.txt|.
   
   These functions are intended for case changing programmatic data in
   places where upper/lower case distinctions are meaningful. One example
   would be automatically generating a function name from user input where
   some case changing is needed. In this situation the input is programmatic,
   not textual, case does have meaning and a language-independent one-to-one
   mapping is appropriate. For example
%   \begin{verbatim}
%     \docAuxCommand*_new_protected:Npn \myfunc:nn #1#2
%       {
%         \docAuxCommand*_set_protected:cpn
%           {
%             user
%             \str_upper_case:f { \tl_head:n {#1} }
%             \str_lower_case:f { \tl_tail:n {#1} }
%           }
%           { #2 }
%       }
%   \end{verbatim}
%   would be used to generate a function with an auto-generated name consisting
%   of the upper case equivalent of the supplied name followed by the lower
%   case equivalent of the rest of the input.
%   
%   These functions should \emph{not} be used for
%   \begin{itemize}
%     \item Caseless comparisons: use \docAuxCommand*{str_fold_case:n} for this
%       situation (case folding is district from lower casing).
%     \item Case changing text for typesetting: see the \docAuxCommand*{tl_lower_case:n(n)},
%       \docAuxCommand*{tl_upper_case:n(n)} and \docAuxCommand*{tl_mixed_case:n(n)} functions which
%       correctly deal with context-dependence and other factors appropriate
%       to text case changing.
%   \end{itemize}
%
%   \begin{texnote}
%     As with all \pkg{expl3} functions, the input supported by
%     \docAuxCommand*{str_fold_case:n} is \emph{engine-native} characters which are or
%     interoperate with \textsc{utf-8}. As such, when used with \pdfTeX{}
%     \emph{only} the Latin alphabet characters A--Z will be case-folded
%     (\emph{i.e.}~the \textsc{ascii} range which coincides with
%     \textsc{utf-8}). Full \textsc{utf-8} support is available with both
%     \XeTeX{} and \LuaTeX{}, subject only to the fact that \XeTeX{} in
%     particular has issues with characters of code above hexadecimal
%     $0\mathrm{xFFF}$ when interacting with \docAuxCommand*{tl_to_str:n}.
%   \end{texnote}
 \end{docCommand}
 
 A common programming task is to convert strings to either uppercase or lowercase equivalents.v
 \begin{texexample}{Converting strings to lower and uppercase}{ex:cases}%TOFIX 
 \ExplSyntaxOn
    \tl_tail:n {TEST} 
   
      \cs_new_protected:Npn \myfunc:nn #1#2
       {
         \cs_set_protected:cpn
           {
             user
             \str_upper_case:f { \tl_head:n {#1} }
             \str_lower_case:f { \tl_tail:n {#1} }
           }
           { #2 }
       }
\docAuxCommand*_new_protected:cpn {yiannis}{Lazarides}
 \ExplSyntaxOff
 \end{texexample}


